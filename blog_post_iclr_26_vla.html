<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- SEO Meta Tags -->
  <title>State of Vision-Language-Action (VLA) Research at ICLR 2026 – Moritz Reuss</title>
  <meta name="description" content="Comprehensive analysis of 164 Vision-Language-Action (VLA) model submissions at ICLR 2026. Discover trends in discrete diffusion VLAs, reasoning models, benchmarks (LIBERO, CALVIN, SIMPLER), and the gap between frontier and academic research.">
  <meta name="keywords" content="VLA models, Vision-Language-Action, ICLR 2026, robot learning, embodied AI, discrete diffusion VLA, VLA benchmarks, LIBERO, CALVIN, SIMPLER, robotics research">
  <meta name="author" content="Moritz Reuss">
  <meta name="robots" content="index, follow">
  <link rel="canonical" href="https://mbreuss.github.io/blog_post_iclr_26_vla.html">
  
  <!-- Open Graph / Social Media Meta Tags -->
  <meta property="og:title" content="State of VLA Research at ICLR 2026">
  <meta property="og:description" content="Analysis of 164 Vision-Language-Action model submissions at ICLR 2026. Learn about discrete diffusion VLAs, reasoning models, and current benchmark trends.">
  <meta property="og:type" content="article">
  <meta property="og:url" content="https://mbreuss.github.io/blog_post_iclr_26_vla.html">
  <meta property="og:image" content="https://mbreuss.github.io/images/blog_header.png">
  <meta property="article:published_time" content="2025-10-10">
  <meta property="article:author" content="Moritz Reuss">
  
  <!-- Twitter Card Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="State of VLA Research at ICLR 2026">
  <meta name="twitter:description" content="Comprehensive analysis of Vision-Language-Action models at ICLR 2026 - discrete diffusion, reasoning VLAs, and benchmark insights.">
  <meta name="twitter:image" content="https://mbreuss.github.io/images/blog_header.png">
  
  <!-- Schema.org Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "headline": "State of Vision-Language-Action (VLA) Research at ICLR 2026",
    "author": {
      "@type": "Person",
      "name": "Moritz Reuss"
    },
    "datePublished": "2025-10-10",
    "dateModified": "2025-10-10",
    "description": "Comprehensive analysis of Vision-Language-Action models at ICLR 2026, covering discrete diffusion, reasoning VLAs, benchmarks, and research trends.",
    "keywords": "VLA models, Vision-Language-Action, robot learning, ICLR 2026, embodied AI, discrete diffusion",
    "image": "https://mbreuss.github.io/images/blog_header.png",
    "url": "https://mbreuss.github.io/blog_post_iclr_26_vla.html"
  }
  </script>
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  
  <style>
    .figure-container {
      max-width: 800px;
      margin: 0 auto;
    }
    .figure-container img {
      width: 100%;
      height: auto;
    }
  </style>

  <!-- Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-ETMW7BYE0C"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-ETMW7BYE0C');
  </script>
</head>

<body>
  <table style="width:100%; max-width:800px; margin:auto;">
    <tr>
      <td>

        <h1 style="text-align:center;">State of VLA Research at ICLR 2026</h1>
        <p style="text-align:center; color:gray;">October 2025 • by Moritz Reuss</p>
        <hr>
        <figure class="figure-container">
          <img src="images/blog_header.png" alt="Vision-Language-Action (VLA) models research overview and trends at ICLR 2026">
        </figure>
        <p>
          ICLR’s open submission policy gives a rare, real-time view of what the community is actually building. 
          This post distills the state of <strong>Vision-Language-Action (VLA) Models</strong> research slice of ICLR 2026: what ‘counts’ as a VLA (and why that definition matters), what people are working on in the VLA field (discrete diffusion, embodied reasoning, new tokenizers), how to read benchmark results in VLA research, and the not-so-invisible frontier gap that sim leaderboards hide.
        </p>

        <p>
          Every year in autumn, the ICLR submission deadline looms over the machine learning community.
          Unlike NeurIPS or ICML, ICLR publicly releases all anonymous submissions a few weeks after the deadline, 
          providing a unique real-time snapshot of ongoing research around the world without the typical six-month delay of other top ML conferences.  
          Given my personal research interests, I wanted to analyze <strong>Vision-Language-Action (VLA) Models</strong> research and share insights from this year's submissions.<br><br>

          In this blog post, I briefly explain what VLAs are, share my findings about current trends and challenges in VLA research, highlighting some interesting papers submitted to ICLR 2026.
          To help researchers and practitioners better understand the current state of VLA research, I've also included a practical guide on how to interpret benchmark results in VLA papers.
          Finally, there is a discussion about the hidden gap between frontier labs and academic research labs in the VLA field—a gap that isn't visible from reading papers alone.<br><br>

          Below is a summary of general research trends with highlighted papers for each category that I found interesting from this year's submissions.
          Please note that this represents only my personal selection and opinion of papers and research, and I may have missed other excellent work.
          If you know of noteworthy papers that I missed, please let me know.<br><br>
          </p>

           <h2>Table of Contents</h2>
        <nav style="background: #f8f9fa; padding: 20px; border-left: 4px solid #4CAF50; margin: 20px 0;">
          <ol style="line-height: 1.8;">
            <li><a href="#definition-vla">What is a Vision-Language-Action Model?</a></li>
            <li><a href="#growth-vla">The Explosive Growth of VLA Research</a></li>
            <li><a href="#key-trends">Summary of Key Trends</a></li>
            <li><a href="#practitioners-guide">Practitioners Guide to Understanding Benchmark Results</a></li>
            <li><a href="#discrete-diffusion">Discrete Diffusion VLAs</a></li>
            <li><a href="#embodied-cot">Reasoning VLAs and Embodied Chain-of-Thought (ECoT)</a></li>
            <li><a href="#tokenizers">New Discrete Tokenizers</a></li>
            <li><a href="#efficient-vla">Efficient VLAs</a></li>
            <li><a href="#RL-vla">RL for VLAs</a></li>
            <li><a href="#vla-video">VLA + Video Prediction</a></li>
            <li><a href="#evaluation">Evaluation and Benchmarking of VLAs</a></li>
            <li><a href="#cross-action-space">Cross-Action-Space Learning</a></li>
            <li><a href="#other">Other Interesting Papers</a></li>
            <li><a href="#frontier-gap">The Hidden Gap Between Frontier and Research VLAs</a></li>
            <li><a href="#summary-outlook">Summary and Outlook</a></li>
          </ol>
        </nav>

        <hr>

        <h2 id="definition-vla">What is a Vision-Language-Action Model?</h2>

        <h2 id="definition-vla">What is a Vision-Language-Action Model?</h2>

      <h3>What Exactly Is a VLA?</h3>
      <figure class="figure-container">
        <img src="images/vla_or_not_chart.png" alt="Flowchart for determining if your model is a VLA">
        <figcaption>Figure 1: Decision flowchart to determine if your model qualifies as a VLA based on my vision-language internet-scale pretraining criteria.</figcaption>
      </figure>
      <p>
        The definition of VLAs is surprisingly contentious, with no clear consensus in the community.
        A <a href="https://arxiv.org/pdf/2510.07077" target="_blank">recent survey paper</a> defines it broadly: 
        "A Vision-Language-Action (VLA) model is a system that takes visual observations and natural language instructions as required inputs and may incorporate additional sensory modalities. 
        It produces robot actions by directly generating control commands."
      </p>

      <p>
        While this is a valid definition, in my personal opinion it misses what I consider the crucial distinguishing feature compared to other multimodal policies: <strong>internet-scale pretraining on some type of vision-language data</strong>.
      </p>

      <p>
        <strong>My personal definition:</strong> A VLA is a model that uses a pretrained backbone, which was trained on large-scale vision-language data and is subsequently trained with an action-generation objective to directly output robot actions.
        This definition also includes Video-Action Policies that use pretrained video-generation models as their backbone. 
        Without internet-scale pretraining, I refer to these as <strong>multimodal policies</strong> rather than VLAs.
        Where it becomes a bit fuzzy is when a model uses a pretrained text encoder like CLIP-text or T5 and a separately pretrained vision encoder like DINOv2 or SigLIP-Vision.
        In my personal opinion, I like to group these as multimodal policies and not VLAs since they are lacking the joint vision-language pretraining.
        For reference I also included a flowchart (Figure 1) to help you determine if your model is a VLA or not based on my definition.
      </p>

      <p>
        This distinction matters because internet-scale pretraining is what theoretically gives VLAs their moat: stronger language-instruction following and better generalization across tasks and environments.
        That's the promise, at least.
        The reality? Most current VLAs still struggle with zero-shot generalization and complex tasks, making them better described as "less dumb multi-modal policies" rather than truly general robot brains.
        But the potential is there and it has many exciting open problems for researchers to tackle.
      </p>

      <figure class="figure-container">
        <img src="images/lbm_flow_chart.png" alt="Taxonomy diagram to determine if your model is a LBM">
        <figcaption>Figure 2: Visual taxonomy showing if your policy is a LBM or not.</figcaption>
      </figure>

      <p>
        <strong>Related and complementary:</strong> Large Behavior Models (LBMs), a term from Toyota Research Institute (TRI) defined in their <a href="https://arxiv.org/pdf/2507.05331" target="_blank">recent paper</a>, represent another category.
        LBMs are robot policies trained on large-scale multitask robotic demonstration data, but they don't require internet-scale vision-language pretraining or a VLM backbone.
        Think of it this way: all VLAs trained on large-scale robotic data are also LBMs, but not all LBMs are VLAs. 
        Together these two terms cover all types of robot foundation policies. 
        I also included a flowchart (Figure 2) to help you determine if your model is a LBM or not based on my understanding
      </p>

      <p>
        What's your personal definition of a VLA? Do you agree with my take on internet-scale pretraining as the key differentiator?  Let me know your thoughts!
      </p>


        <h2 id="growth-vla">The Explosive Growth of VLA Research</h2>

        <div style="display: flex; gap: 20px; align-items: flex-start; margin: 20px 0; flex-wrap: wrap;">
          <div style="flex: 1; min-width: 350px;">
        </div>
        <p>
          The Vision-Language-Action field has experienced remarkable growth over the past two years. 
          Based on keyword searches on OpenReview, the numbers tell an interesting story based on ICLR submissions openreview keyword search for "Vision-Language-Action":
        </p>
        
        <ul>
          <li><strong>ICLR 2024:</strong> Only 1 rejected submission with "Vision-Language-Action" keyword</li>
          <li><strong>ICLR 2025:</strong> 6 accepted papers and 3 rejected submissions with the same keyword</li>
          <li><strong>ICLR 2026:</strong> 164 submissions with the same keyword—an 18x increase in just one year!</li>
        </ul>

        <p>
          This exponential growth trajectory shows that Vision-Language-Action models are rapidly gaining popularity with many new people from other domains like Vision coming into the exciting field of robot learning. 
          Given this trend, I'm both excited and a bit terrified to review the projected 2,100+ VLA submissions at ICLR 2027, though I suspect the growth rate may stabilize as the field matures. 
        </p>

        <!-- Growth Visualization -->
          <div style="flex: 0 0 320px; min-width: 280px; max-width: 350px;">
            <svg viewBox="0 0 450 350" style="width: 100%; max-width: 350px; height: auto; background: #f8f9fa; border-radius: 8px; padding: 15px; box-sizing: border-box;">
              <!-- Title -->
              <text x="225" y="25" text-anchor="middle" style="font-size: 16px; font-weight: bold; fill: #333;">
                VLA Submissions Growth at ICLR
              </text>
              
              <!-- Y-axis -->
              <line x1="60" y1="40" x2="60" y2="300" stroke="#666" stroke-width="2"/>
              <!-- X-axis -->
              <line x1="60" y1="300" x2="410" y2="300" stroke="#666" stroke-width="2"/>
              
              <!-- Y-axis labels -->
              <text x="45" y="305" text-anchor="end" style="font-size: 11px; fill: #666;">0</text>
              <text x="45" y="255" text-anchor="end" style="font-size: 11px; fill: #666;">250</text>
              <text x="45" y="205" text-anchor="end" style="font-size: 11px; fill: #666;">500</text>
              <text x="45" y="155" text-anchor="end" style="font-size: 11px; fill: #666;">750</text>
              <text x="45" y="105" text-anchor="end" style="font-size: 11px; fill: #666;">1000</text>
              <text x="45" y="55" text-anchor="end" style="font-size: 11px; fill: #666;">1250</text>
              
              <!-- Grid lines -->
              <line x1="60" y1="250" x2="410" y2="250" stroke="#ddd" stroke-width="1" stroke-dasharray="4"/>
              <line x1="60" y1="200" x2="410" y2="200" stroke="#ddd" stroke-width="1" stroke-dasharray="4"/>
              <line x1="60" y1="150" x2="410" y2="150" stroke="#ddd" stroke-width="1" stroke-dasharray="4"/>
              <line x1="60" y1="100" x2="410" y2="100" stroke="#ddd" stroke-width="1" stroke-dasharray="4"/>
              
              <!-- Bars -->
              <!-- 2024: 1 submission (barely visible) -->
              <rect x="80" y="299" width="50" height="1" fill="#4CAF50" opacity="0.8"/>
              <text x="105" y="315" text-anchor="middle" style="font-size: 12px; font-weight: bold; fill: #333;">2024</text>
              <text x="105" y="290" text-anchor="middle" style="font-size: 14px; font-weight: bold; fill: #4CAF50;">1</text>
              
              <!-- 2025: 9 submissions -->
              <rect x="160" y="295.4" width="50" height="4.6" fill="#4CAF50" opacity="0.8"/>
              <text x="185" y="315" text-anchor="middle" style="font-size: 12px; font-weight: bold; fill: #333;">2025</text>
              <text x="185" y="287" text-anchor="middle" style="font-size: 14px; font-weight: bold; fill: #4CAF50;">9</text>
              
              <!-- 2026: 164 submissions -->
              <rect x="240" y="216.8" width="50" height="83.2" fill="#2E7D32" opacity="0.9"/>
              <text x="265" y="315" text-anchor="middle" style="font-size: 12px; font-weight: bold; fill: #333;">2026</text>
              <text x="265" y="205" text-anchor="middle" style="font-size: 16px; font-weight: bold; fill: #2E7D32;">164</text>
              
              <!-- 2027: ~1000 submissions (dashed outline bar going way up!) -->
              <rect x="320" y="94" width="50" height="206" fill="none" stroke="#999" stroke-width="2" stroke-dasharray="5,5" opacity="0.6"/>
              
              <!-- Arrow pointing up indicating it goes off-chart -->
              <path d="M 345 94 L 345 50" stroke="#999" stroke-width="2" stroke-dasharray="5,5" opacity="0.4"/>
              <polygon points="345,45 350,55 340,55" fill="#999" opacity="0.4"/>
              
              <text x="345" y="315" text-anchor="middle" style="font-size: 12px; font-weight: bold; fill: #333;">2027</text>
              <text x="345" y="82" text-anchor="middle" style="font-size: 18px; font-weight: bold; fill: #999;">1000+?</text>
              <text x="345" y="330" text-anchor="middle" style="font-size: 10px; fill: #999; font-style: italic;">projected</text>
              
              <!-- Growth arrow from 2025 to 2026 -->
              <path d="M 210 295 Q 220 270, 240 230" stroke="#FF6B6B" stroke-width="2" fill="none" stroke-dasharray="5,3"/>
              <polygon points="240,225 245,235 235,235" fill="#FF6B6B"/>
              
              <!-- 18x growth annotation (between 2025 and 2026) -->
              <text x="225" y="265" text-anchor="middle" style="font-size: 11px; fill: #FF6B6B; font-style: italic;">
                18x growth!
              </text>
              
              <!-- Future growth arrow (dotted) - dramatic upward curve from 2026 to 2027 -->
              <path d="M 290 230 Q 310 180, 320 120" stroke="#FF6B6B" stroke-width="2" fill="none" stroke-dasharray="2,4" opacity="0.5"/>
              <polygon points="320,115 325,125 315,125" fill="#FF6B6B" opacity="0.5"/>
              
              <!-- Future growth annotation -->
              <text x="310" y="160" text-anchor="middle" style="font-size: 10px; fill: #FF6B6B; font-style: italic;">
                exponential?
              </text>
            </svg>
          </div>
        </div>


        <h2 id="key-trends">Summary of Key Trends</h2>
        <p>After scanning most ICLR 2026 submissions with the VLA keyword, I identified the following key trends in VLA research with a lot of overlap in between these categories:</p>
        <ul>
          <li><a href="#discrete-diffusion">Discrete Diffusion VLAs</a></li>
          <li><a href="#embodied-cot">Reasoning VLAs and Embodied Chain-of-Thought</a></li>
          <li><a href="#tokenizers">New Discrete Tokenizers</a></li>
          <li><a href="#efficient-vla">Efficient VLAs</a></li>
          <li><a href="#RL-vla">RL for VLAs</a></li>
          <li><a href="#vla-video">VLA + Video Prediction</a></li>
          <li><a href="#evaluation">Evaluation and Benchmarking of VLAs</a></li>
          <li><a href="#cross-action-space">Cross-Action-Space Learning</a></li>
          <li><a href="#other">Other Interesting Papers</a></li>
        </ul>

        <h2 id="practitioners-guide">Practitioners Guide to Understanding Benchmark Results in VLA Research</h2>
        <figure class="figure-container">
          <img src="images/vla_sim_setups.png" alt="VLA Simulation Setups Overview">
          <figcaption>Figure 2: Overview of the most popular VLA simulation benchmarks: <a href="https://arxiv.org/abs/2306.03310" target="_blank">LIBERO</a>, <a href="https://openreview.net/pdf?id=LZh48DTg71" target="_blank">SIMPLER</a> and <a href="https://arxiv.org/abs/2112.03227" target="_blank">CALVIN</a> 
          </figcaption>
        </figure>

        <p>
          I want to provide a quick guide for practitioners on how to interpret benchmark results in VLA papers.
          So you are reading a new VLA paper and want to understand if the claimed results are actually good or not?
          In order to better understand the VLA results, it is important to have some context on the current state of popular VLA benchmarks and what good performance looks like.<br><br>

          Given the current benchmark there is no clear statement possible to say which model is the best, as most papers only compare against each other on the usual simulation benchmarks.
          90% of papers mentioned in this post all test in either <a href="https://arxiv.org/abs/2306.03310" target="_blank">LIBERO</a>, <a href="https://openreview.net/pdf?id=LZh48DTg71" target="_blank">SIMPLER</a> or <a href="https://arxiv.org/abs/2112.03227" target="_blank">CALVIN</a>.<br><br>
          LIBERO is basically solved and showing 99% vs 98% is not very helpful and you don't need VLAs and large-scale pretraining to get competitive results.
          E.g. all mentioned concurrent discrete diffusion policies below test on LIBERO and all achieve between 95-98% average over the 4 versions (Goal, Spatial, Long, Object).
          It's also funny to note that LIBERO was designed as a life-long learning benchmark but 99% of models reporting results on it just train on the full dataset and don't do any continual learning.
          Based on these results it is impossible to say which model is better, as they are all very close to the ceiling anyways and you don't need internet scale pretraining to solve these.<br><br>

          Rule of thumb for getting a impression a VLA is decent or near sota levels on these: LIBERO: >95% is expected for Spatial, Goal and Object, 90-95% is required for Long and 90, lower than 90% is only fine for static-camera-only or few-shot stuff.
          A properly tuned Diffusion Policy can get you there without VLAs although the most cited DP baselines is showing worse results. 
          CALVIN is also almost saturated by current sota models like <a href="https://arxiv.org/pdf/2509.04996">FLOWER</a>.
          The benchmark has 3 main version: D (train on setup D and test on setup D), ABC (train on A,B,C and test on D) and ABCD (train on A,B,C, D and test on D).
          ABC is the most relevant one, as it tests generalization to unseen setups, ABCD test how well methods benefit from more diverse data and D tests fine-tuning.
          For CALVIN higher than 4 score for ABC is standard now and above 4.5 is sota regime. 
          For the D version 3.75 is standard and above 4 is very good. 
          For the ABCD version results above 4.5 are relevant.<br><br>

          SIMPLER is hard to interpret across setups; success spans 40–99% on Bridge, making cross-paper comparison noisy.
          For the Google Robot version current sota models achieve around 70%-80% success rate. 
          Somehow, <a href="https://arxiv.org/abs/1909.12271" target="_blank">RLBench</a> (the most popular 3d policy benchmark) has gained more popularity in VLA benchmarking, but all VLAs are still far away from 3D SOTA methods like <a href="https://arxiv.org/abs/2402.10885" target="_blank">3DDA</a>. 
          Most VLA policies try to avoid comparing against all relevant 3D baselines for some reason. 
          Any real world results are very important and more is better, as sim-only is hard to trust especially with VLA papers that use models with 7B+ parameters and are very good in overfitting successfully on these benchmarks.<br><br>
        </p>

        <hr>

        <h2 id="discrete-diffusion">1. Discrete Diffusion VLAs</h2>
        <figure class="figure-container">
          <img src="images/discrete_diff_comp.png" alt="Discrete Diffusion VLA Architecture">
          <figcaption>Figure 3: Overview of discrete diffusion paradigm for action generation. Image from <a href="https://openreview.net/attachment?id=YWeNCMxdhM&name=pdf">DISCRETE DIFFUSION VLA</a></figcaption>
        </figure>
        <p>
          Given the success of discrete diffusion models in other domains like text (e.g. <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/eb0b13cc515724ab8015bc978fdde0ad-Paper-Conference.pdf">(MDLM</a>) and VLMs (e.g. <a href="https://arxiv.org/pdf/2505.16933">LLaDA-V</a>), it is no surprise that this trend is also making its way into VLA research. 
          Why discrete diffusion? Compared to autoregressive models, diffusion models can generate sequences in parallel, which is a big advantage for discrete action token generation. 
          Instead of having to run your policy 100 times, you can generate long action sequences in a few forward passes. 
          In addition, you can combine it with ideas from Embodied Chain-of-Thought (see next section) to generate sub-goals and reasoning together with actions in parallel. 
          This tackles some of the biggest limitations of ECoT from prior work, which was extremely slow due to the autoregressive nature of the VLM models.
          Current attempts for discrete Diffusion either finetune an autoregressive VLM with discrete diffusion because the variety of discrete diffusion VLMs is very limited.
          Other work use LLaDA-V as a pretrained backbone with good success. 
          Below are 4 concurrent papers that all propose different discrete diffusion VLAs with promising results on LIBERO and SIMPLER.
        </p>

        <p><strong>Notable papers:</strong></p>
        <ul>
          <li>
            <a href="https://openreview.net/attachment?id=YWeNCMxdhM&name=pdf">
              DISCRETE DIFFUSION VLA: BRINGING DISCRETE DIFFUSION TO ACTION DECODING IN VISION-LANGUAGE-ACTION POLICIES
            </a><br>
            <em>TL;DR:</em> Take OpenVLA and apply Discrete Diffusion Action Prediction for fast action chunk-based generation of discrete action tokens. Also proposes adaptive decoding for inference.
            Strong results on LIBERO + SIMPLER.
          </li>
          <li>
            <a href="https://openreview.net/attachment?id=2rxgospB5s&name=pdf">
              dVLA: DIFFUSION VISION-LANGUAGE-ACTION MODEL WITH MULTIMODAL CHAIN-OF-THOUGHT
            </a><br>
            <em>TL;DR:</em> Another Discrete Diffusion VLA using Co-Generation for Future Frames and text + actions given the advantage of fast parallel sampling of Discrete Diffusion over AR models. 
            Basically ECoT + Discrete Diffusion done well. 
            Also good results in LIBERO + real world experiments.
          </li>
          <li>
            <a href="https://openreview.net/attachment?id=mNya9d1DA2&name=pdf">
              DIVA: DISCRETE DIFFUSION VISION-LANGUAGE-ACTION MODELS FOR PARALLELIZED ACTION GENERATION
            </a><br>
            <em>TL;DR:</em> Another discrete Diffusion VLA that also focuses on how to substitute tokens during inference for better performance.
          </li>
          <li>
            <a href="https://openreview.net/attachment?id=UvQOcw2oCD&name=pdf">
              UNIFIED DIFFUSION VLA: VISION-LANGUAGE-ACTION MODEL VIA JOINT DISCRETE DENOISING DIFFUSION PROCESS
            </a><br>
            <em>TL;DR:</em> Generates future frames and discrete actions together with block-wise causal masking. Results on CALVIN, LIBERO and SIMPLER are good.
          </li>
        </ul>

        <h2 id="embodied-cot">2. Reasoning VLAs and Embodied Chain-of-Thought (ECoT)</h2>


        <p>
          Reasoning holds strong promise for improving the generalization and performance of VLAs, which still struggle with complex tasks and out-of-distribution scenarios.
          Inspired by the success of Chain-of-Thought prompting in LLMs, there is growing interest in applying similar ideas to VLAs.
          The core idea is to bridge action generation with intermediate visual and textual reasoning steps that help the VLA better ground and understand the task and environment.
          These reasoning traces are also more interpretable and can be used for debugging and understanding a VLA’s behavior.
        </p>

        <p>
          Since <a href="https://arxiv.org/abs/2407.08693" target="_blank">the first ECoT paper</a> (CoRL 2024), interest has grown in combining spatially grounded reasoning with action prediction to improve VLAs.
          By combining subtasks, bounding-box predictions for task-relevant objects, and 2D motion trajectories, VLMs learn better representations for embodied tasks and show improved performance on generalization benchmarks.
          Prior analyses of <a href="https://arxiv.org/abs/2505.08243" target="_blank">ECoT training</a> indicate that these objectives help bridge the representation gap between VLM static pretraining and robotics tasks.
          However, a key limitation of prior ECoT work is the autoregressive nature of VLAs and the increased token count, which results in slower training and inference.
        </p>

        <p>
          Overall, it remains an open question how to best implement grounded reasoning for VLAs.
          Recent work has explored additional modalities like depth prediction in <a href="https://arxiv.org/pdf/2508.07917" target="_blank">MolmoAct</a>.
          A major bottleneck is the limited availability of diverse training data: many ECoT studies still rely on the same BRIDGE and LIBERO labeled datasets.
          More diverse datasets with more complex tasks and environments are needed to push this direction further; however, labeling large-scale sources like DROID is tough.

        </p>

        <p><strong>Notable papers:</strong></p>
        <ul>
          <li>
            <a href="https://openreview.net/attachment?id=sFO9d6XSlf&name=pdf">
              ACTIONS AS LANGUAGE: FINE-TUNING VLMS INTO VLAS WITHOUT CATASTROPHIC FORGETTING
            </a><br>
            <em>TL;DR:</em> Instead of directly fine-tuning VLMs with discrete action tokens to become VLAs, which results in catastrophic forgetting, they relabel robot datasets with subtasks, actions as text and intermediate motion-planning like "move left".
            This training method is able to bridge the domain gap of VLMs without reducing performance on other VQA benchmarks from pretraining. 
            Finally cheap LORA finetuning is enough to get strong results for action prediction while maintaining VLM reasoning capabilities.
          </li>
          <li>
            <a href="https://openreview.net/attachment?id=tsxwloasw5&name=pdf">
              VISION-LANGUAGE-ACTION INSTRUCTION TUNING: FROM UNDERSTANDING TO MANIPULATION
            </a><br>
            <em>TL;DR:</em> InstructVLA proposes a two-stage Vision-Language-Action Instruction Tuning pipeline that tries to preserves a pretrained VLM’s multimodal reasoning while adding precise manipulation: (1) pretrain an action expert and latent action interface, then (2) instruction-tune a MoE-adapted VLM to switch between textual reasoning and latent action generation.
                   It puts emphasis on decoupling multimodal reasoning and action generation to avoid catastrophic forgetting with an action expert, and introduces an instructed-based SIMPLER benchmark to test instruction-following VLAs.
          
          <li>
            <a href="https://openreview.net/attachment?id=i5wlozMFsQ&name=pdf">EMBODIED-R1: REINFORCED EMBODIED REASONING FOR GENERAL ROBOTIC MANIPULATION</a><br>
            <em>TL;DR:</em> R1 is a pointing VLM for embodied reasoning: it trains Qwen2.5-VL base with a two-stage Reinforced Fine-Tuning (RFT) curriculum on the new Embodied-Points-200K dataset.
            It uses an embodiment-agnostic intermediate REG (point to the referred object), RRG (point to a relation-defined place), OFG (point to a functional part, e.g., a handle), VTG (output a sequence of points as a visual trace/trajectory).
            Strong performance on embodied benchmarks and pointing ones with good generalization to SIMPLER as planner with intermediate waypoints.
          </li>

          <li>
            <a href="https://openreview.net/attachment?id=IBJtOltTbx&name=pdf">
              HYBRID TRAINING FOR VISION-LANGUAGE-ACTION MODELS
            </a><br>
            <em>TL;DR:</em> Decomposes ECoT pretraining in several subtasks of think act and follow, that enable to maintain the performance benefits with fast inference. 
            Similar findings, that Co-Training with ECoT stuff results in better representations for action prediction.
          </li>
        </ul>

        <h2 id="tokenizers">3. New Tokenizers</h2>
        <figure class="figure-container">
          <img src="images/faster_tokenizer.png" alt="FASTer Tokenizer Overview">
          <figcaption>Figure 4: FASTer Tokenizer Overview. The tokenizer combines RVQ quantization with frequency and time domain losses. Image from <a href="https://openreview.net/attachment?id=k6nTUFoqeT&name=pdf">FASTer</a>.</figcaption>
        </figure>
        <p>
          We command robots with high-frequency, continuous control values (e.g., joint angles, gripper state). Vision-Language models, however, operate most effectively on discrete tokens. Naively fine-tuning a VLM to regress continuous actions tends to underperform and often induces catastrophic forgetting, because the new objective misaligns with the model’s pretrained representations.
        </p>
        
        <p></p>
          The core idea of these tokenizers is to convert continuous action sequences into compact discrete tokens that a VLM can predict—retaining accuracy and smoothness while minimizing compute and integration overhead. 
          An ideal action tokenizer is fast, achieves a high compression ratio for long action-chunks, produces smooth long-horizon outputs, and drops into existing VLM architectures without modification.
        </p>

        <p>
          Prior work used discrete binning (e.g., <a href="https://arxiv.org/pdf/2212.06817" target="_blank">RT-1</a>) and <a href="https://arxiv.org/pdf/2403.03181" target="_blank">VQ-VAE</a> codebooks, but both struggle with either coarse precision or long-sequence efficiency. 
          <a href="https://arxiv.org/abs/2501.09747" target="_blank">FAST</a> introduced action-chunk tokenizers tailored for VLA prediction, demonstrating that discrete tokens can replace more complex diffusion/flow experts for integration. 
          Building on this, newer tokenizers submitted to ICLR combine Residual Vector Quantization (RVQ) (e.g., <a href="https://arxiv.org/abs/2107.03312" target="_blank">SoundStream</a>) for higher compression, spline-based parameterizations inspired by <a href="https://arxiv.org/pdf/2506.06072" target="_blank">BEAST tokenizer</a> for smooth, long trajectories, and DCT-style objectives (as in <a href="https://arxiv.org/abs/2501.09747" target="_blank">FAST</a>) to bias toward low-frequency, physically plausible motion. 
          I’m excited to test these tokenizers myself when they release.
        </p>


        <p><strong>Notable papers:</strong></p>
        <ul>
          <li>
            <a href="https://openreview.net/attachment?id=k6nTUFoqeT&name=pdf">
              FASTER: TOWARD POWERFUL AND EFFICIENT AUTOREGRESSIVE VISION–LANGUAGE–ACTION MODELS WITH LEARNABLE ACTION TOKENIZER AND BLOCK-WISE DECODING
            </a><br>
            <em>TL;DR:</em> Introduces a novel discrete action tokenizer called FASTer, that combines Residual Vector Quantification (RVQ) with a frequency L1 loss using DCT and time domain L1 loss for improved performance. 
            Also patchifies action tokens along the temporal axis and grouped action dimension axis (e.g. base motion, arm joints). It has a higher compression ratio than <a href="https://arxiv.org/abs/2501.09747" target="_blank">FAST</a> and results on SIMPLER and LIBERO are strong.
          </li>
          <li>
            <a href="https://openreview.net/attachment?id=CuzTXLB7Jz&name=pdf">
              OMNISAT: COMPACT ACTION TOKEN, FASTER AUTOREGRESSION FOR VISION-LANGUAGE-ACTION MODELS
            </a><br>
            <em>TL;DR:</em> Another tokenizer for VLAs that uses our <a href="https://arxiv.org/pdf/2506.06072" target="_blank">BEAST</a> paper idea of B-Splines for compact representation of continuous action chunks.
            It uses a two stage encoding process: First, aligning the different action chunk lengths of different embodiments into a normalized, fixed-length representation.
            Next, it uses a B-Spline based encoder to get a compact representation of the normalized action chunk. Finally, a VQ-VAE is used to get discrete tokens.
            Results on LIBERO and SIMPLER are good and across all benchmarks improves upon both <a href="https://arxiv.org/abs/2501.09747" target="_blank">FAST</a> and <a href="https://arxiv.org/pdf/2506.06072" target="_blank">BEAST</a>.
          </li>
        </ul>
      

        <h2 id="efficient-vla">4. Efficient VLAs</h2>
        <p>As someone working on this topic myself, I understand the pain of trying to train and run larger VLAs on limited compute setups. 
          Thus, efficient VLAs are always relevant. Especially, since this gives labs with limited compute access the chance to work on VLAs too.
          There are several interesting papers this year that try to tackle this problem with different approaches.
          Generally, one can divide them into two categories: try to make training and models more efficient by making smaller VLAs or better tokenizers etc. 
          Otherwise, a lot of papers focus on making inference more efficient by using better quantization, distillation or similar ideas. 
        </p>

        <p><strong>Notable papers:</strong></p>
        <ul>
          <li>
            <a href="https://openreview.net/attachment?id=bsXkBTZjgY&name=pdf">
              HYPERVLA: EFFICIENT INFERENCE IN VISION- LANGUAGE-ACTION MODELS VIA HYPERNETWORKS
            </a><br>
            <em>TL;DR:</em> HyperVLA uses hypernetworks to generate small task-specific policies conditioned on language instructions and initial images, dramatically reducing inference cost while maintaining performance by only activating the compact generated policy during execution instead of a big VLA model.
          </li>

          <li>
            <a href="https://openreview.net/attachment?id=TpL2nXanru&name=pdf">
              AUTOQVLA: NOT ALL CHANNELS ARE EQUAL IN VISION-LANGUAGE-ACTION MODEL’S QUANTIZATION
            </a><br>
            <em>TL;DR:</em> Analysis quantization of OpenVLA and proposes improved quantization method for maintaining performance with only 30% of the original VRAM requirements.
          </li>
        </ul>

        <h2 id="RL-vla">5. RL for VLAs</h2>
        <p>Finetuning VLAs to go from 70-80% success rate in the real world towards 99% is still an open problem.  
          There is a lot of hope for RL Finetuning to close this gap. 
          While a lot of attempts have been made before, no method has established itself as the go-to method yet.
          This year there are several interesting papers that try to tackle this problem with different approaches.
        </p>

        <p><strong>Notable papers:</strong></p>
        <ul>
          <li>
            <a href="https://openreview.net/attachment?id=eUGoqrZ6Ea&name=pdf">
              SELF-IMPROVING VISION-LANGUAGE-ACTION MODELS WITH DATA GENERATION VIA RESIDUAL RL
            </a><br>
            <em>TL;DR:</em> Residual RL method that collects more data with frozen VLA and small residual policy. The residual interventions are used to get more high quality data with recovery behavior.
            Finally the VLA is finetuned using SFT. Results on LIBERO achieve 99%.
            </li>

          <li>
            <a href="https://openreview.net/attachment?id=TpL2nXanru&name=pdf">
              PROGRESSIVE STAGE-AWARE REINFORCEMENT FOR FINE-TUNING VISION-LANGUAGE-ACTION MODELS
            </a><br>
            <em>TL;DR:</em> The method breaks robot tasks into semantic stages (Reach→Grasp→Transport→Place) and assigns rewards to each stage instead of the whole trajectory. It uses STA-TPO for offline preference learning and STA-PPO for online reinforcement learning, both operating at the stage level.
            Results on Bridge SIMPLER of 98%. 
          </li>
        </ul>

        <h2 id="vla-video">6. VLA + Video Prediction</h2>
        <figure class="figure-container">
          <img src="images/video_policy_header.png" alt="Video Pretraining for VLAs">
          <figcaption>Figure 5: Example Pretraining Paradigm from <a href="https://openreview.net/attachment?id=DdrsHWobR1&name=pdf">DISENTANGLED ROBOT LEARNING Paper</a><br>
          </figcaption>
        </figure>
        <p>Started by strong results from <a href="https://arxiv.org/pdf/2312.13139" target="_blank">GR-1 paper</a> from ICLR 2024 that showed the potential of video-based policies, there is a growing interest in this subfield.
          These types of policies can be divided into two general categories: starting by a VLM, that optionally has been trained on image generation and continue training it with future frame and action prediction. 
          Otherwise several papers instead start from a Video Foundation Model and modify it to also generate actions. 
          Since most state-of-the-art video foundation models are diffusion/flow-based these policies typically struggle with slow inference speed. 
          Overall, the results demonstrate that video generation and the required physics understanding and language grounding are a very useful prior for robot learning.
          Compared to VLAs initialized from VLMs this subfield is way less popular and I hope to see more research in this direction.
          What holds back more research in this direction is the high requirements for finetuning sota video models like WAN even compared to finetuning VLM for VLAs.
        </p>

        <p><strong>Notable papers:</strong></p>
        <ul>
          <li>
            <a href="https://openreview.net/attachment?id=DdrsHWobR1&name=pdf">
              DISENTANGLED ROBOT LEARNING VIA SEPARATE FORWARD AND INVERSE DYNAMICS PRETRAINING
            </a><br>
            <em>TL;DR:</em> Introduces a novel approach to robot learning by pretraining separate forward and inverse dynamics models pretraining.
            In the second stage they are combined again for a coupled finetuning of the policy. Results on CALVIN are good on SIMPLER decent.
          </li>
          
          <li>
            <a href="https://openreview.net/attachment?id=PklMD8PwUy&name=pdf">UNIFIED VISION–LANGUAGE–ACTION MODEL</a><br>
            <em>TL;DR:</em> Models vision, language, and actions as a single interleaved stream of discrete tokens (VQ image tokens + FAST/DCT action tokens) and trains one autoregressive 8.5B VLA.
            UniVLA has two training stages for converting the VLM into a VLA: First post-train VLM with text and images to predict future frames, next is the fine-tuning stage with vision and action token prediction. 
            Main emphasis on the post-training stage to better align VLM representations with robot tasks. 
            Results on CALVIN, LIBERO, and SimplerEnv-Bridge are strong. 
          </li>

          <li>
            <a href="https://openreview.net/attachment?id=wPEIStHxYH&name=pdf">
              COSMOS POLICY: FINE-TUNING VIDEO MODELS FOR VISUOMOTOR CONTROL AND PLANNING
            </a><br>
            <em>TL;DR:</em> Finetunes the Cosmos Video Foundation Model from NVIDIA for action prediction. 
            Core idea is to inject additional modalities like future action chunks or value function estimations into latent token sequence. 
            Results on LIBERO are good and they also have real world comparisons against <a href="https://www.physicalintelligence.company/download/pi0.pdf" target="_blank">Pi0</a>.5.
          </li>
        </ul>

        <h2 id="evaluation">7. Evaluation and Benchmarking of VLAs</h2>
        <figure class="figure-container">
          <img src="images/roboarena_real2sim.png" alt="ROBOTARENA Overview">
          <figcaption>Figure 6: Overview of real2sim ROBOTARENA benchmark. 
            Image from <a href="https://openreview.net/attachment?id=OutljIofvS&name=pdf">ROBOTARENA ∞</a>.</figcaption>
        </figure>
        <p>
          As mentioned above, the current state of VLA benchmarks is quite saturated and it is hard to judge which model is actually better given the limited number of benchmarks and the fact that most papers only compare against a few other baselines.
          Luckily, there are several submission that try to bridge this gap by introducing new benchmarks for VLAs.
          Other ideas include real2sim world models to test policies in generative environments. 
          While I don't think these ideas are on a good enough level yet to be used as real alternatives yet, it's a very exciting research area that I hope to see more progress in the future.
        </p>

        <p><strong>Notable papers:</strong></p>
        <ul>
          <li>
            <a href="https://openreview.net/attachment?id=OutljIofvS&name=pdf">
              ROBOTARENA ∞: UNLIMITED ROBOT BENCHMARKING VIA REAL-TO-SIM TRANSLATION
            </a><br>
            <em>TL;DR: </em> Introduces a new benchmarking framework inspired by the <a href="https://robo-arena.github.io/" target="_blank">RoboArena</a> work for real2sim setup. 
            It provides automatic environment construction and evaluation using physics engine, real-2-sim translation and human feedback. 
            They use real2sim translation pipeline with a bunch of foundation models and differentiable rendering and VLM-based task progression scores.
            Seems very interesting on first glance and I am excited to try it out myself. 
          </li>

          <li>
            <a href="https://openreview.net/attachment?id=tQJYKwc3n4&name=pdf">
              ROBOCASA365: A LARGE-SCALE SIMULATION FRAMEWORK FOR TRAINING AND BENCHMARKING GENERALIST ROBOTS
            </a><br>
            <em>TL;DR:</em> Extends the initial RoboCasa sim and benchmark setup with a very diverse setup of 365 tasks across 2k+ kitchen scenes and more than 2k hours of teleop data. 
            The task setups look great and the data scale is also promising I just wish they tested more baseline policies than just 3. 
          </li>

          <li>
            <a href="https://openreview.net/attachment?id=hidBHy1CAw&name=pdf">
              WORLDGYM: WORLD MODEL AS AN ENVIRONMENT FOR POLICY EVALUATION
            </a><br>
            <em>TL;DR:</em> WorldGym proposes using an action-conditioned video generation model (world model) as an environment for evaluating robot policies, where policies are rolled out in the generated world and evaluated by a vision-language model that provides rewards. 
          </li>
        </ul>

        <h2 id="cross-action-space">8. Cross-Action-Space-Learning</h2>
        <figure class="figure-container">
          <img src="images/xvla_cross_embedding.png" alt="X-VLA Overview">
          <figcaption>Figure 7: Different Paradigms for VLAs to handle different action spaces.
            Image from <a href="https://openreview.net/attachment?id=kt51kZH4aG&name=pdf">X-VLA</a>.</figcaption>
        </figure>

        <p>
          Most VLAs still avoid pretraining on diverse action spaces, given the difficulties of getting any positive transfer results.
          Thus, it is a very exciting research area for current VLAs to improve. 
          In addition, there is a growing interesting in using human egocentric videos with action labels for pretraining VLAs. 
          Datasets like <a href="https://arxiv.org/pdf/2505.11709" target="_blank">EgoDex</a> released earlier this year now enable more research in this direction. 
          Several interesting papers are submitted this year that try to tackle this problem with different approaches.
          They either focus on architecture details of VLAs to better handle heterogeneous action spaces or use additional abstractions like motion in image-space to get better transfer.
          It's noteworthy that the recent <a href="https://arxiv.org/pdf/2510.03342" target="_blank">Gemini Robotics 1.5</a> release from DeepMind indicates that a unreleased technique called motion transfer works for them to get positive zero-shot task transfer inbetween action spaces.
          So maybe this is just a data and model scale question. Nevertheless, more research is needed to better understand and tackle these issues.
        </p>

        <p><strong>Notable papers:</strong></p>
        <ul>
          <li>
            <a href="https://openreview.net/attachment?id=kt51kZH4aG&name=pdf">
              X-VLA: SOFT-PROMPTED TRANSFORMER AS SCALABLE CROSS-EMBODIMENT VISION-LANGUAGE-ACTION MODEL
            </a><br>
            <em>TL;DR:</em> Tackles cross-action-space learning using soft-prompting tokens for different datasets. 
            These soft-prompt tokens are learnable readout-tokens for the VLA. 
            Results on LIBERO, CALVIN and SIMPLER RoboTwin and VLABench are all very good. Also very insightful scaling analysis.
          </li>
          <li>
            <a href="https://openreview.net/attachment?id=wPEIStHxYH&name=pdf">
              XR-1: TOWARDS VERSATILE VISION-LANGUAGE-ACTION MODELS VIA LEARNING UNIFIED VISION-MOTION REPRESENTATIONS
            </a><br>
            <em>TL;DR:</em>XR-1 introduces Unified Vision-Motion Codes (UVMC), a discrete latent representation that jointly encodes both visual dynamics and robotic motion using a dual-branch VQ-VAE with a shared codebook. This enables better co-pretraining from human and robot demonstrations.
            Tested on real world experiments against Groot-N.1.5 and <a href="https://www.physicalintelligence.company/download/pi0.pdf" target="_blank">Pi0</a> with good results.
          </li>
          <li>
            <a href="https://openreview.net/attachment?id=TX3oGD99CJ&name=pdf">
              HIMOE-VLA: HIERARCHICAL MIXTURE-OF-EXPERTS FOR GENERALIST VISION–LANGUAGE–ACTION POLICIES
            </a><br>
            <em>TL;DR:</em> Substitutes Pi-Style Action Expert with Hierarchical Mixture-of-Experts Transformer to better adapt to new embodiments.
            It interleaves standard blocks with two types of MoE Blocks: Action-Space MoEs and Heterogeneity Balancing MoEs to better handle different action spaces.
            Improves upon <a href="https://www.physicalintelligence.company/download/pi0.pdf" target="_blank">Pi0</a> across a range of experiments. 
          </li>
        </ul>

        <h2 id="other">9. Other Interesting Papers</h2>
        <p>
          There are several other interesting papers that don’t fit neatly into the categories above but are worth mentioning.
          They explore varied aspects of VLA design, from the choice of VLM backbone to adding memory modules into the policy.
          I’m especially interested in memory: most VLAs encode only the current image and ignore prior timesteps, which is a major limitation for many tasks.
        </p>
        <p>
          Naively feeding long histories, into VLAs often backfires: models overfit to demonstrator-specific trajectories, and during rollouts the agent rarely encounters the same state sequences, leading to large performance drops.
          By contrast, memory modules that aggregate and compress past context (rather than memorizing it) look promising.
          Hopefully this makes learned policies more robust to distribution shift while preserving the temporal cues needed for long-horizon control.
        </p>

        <p>
          Another cool work I wanted to highlight is the challenge of composing multiple policies at test time to improve performance.
          Diffusion and flow-based VLAs are required for this, since their energy-based formulation allows combining multiple models by summing their scores.
          This is a promising direction to improve performance without training, and I hope to see more. 
        </p>

        <p><strong>Notable papers:</strong></p>
        <ul>
          <li>
            <a href="https://openreview.net/attachment?id=KcJ9U0x6kO&name=pdf">
              HAMLET: SWITCH YOUR VISION-LANGUAGE-ACTION MODEL INTO A HISTORY-AWARE POLICY
            </a><br>
            <em>TL;DR:</em> Introduces a plug-and-play style memory module with moment tokens to capture temporal stuff from prior timesteps. A proposed memory module aggregates tokens over time to enable history-conditioned prediction.
          </li>
          <li>
            <a href="https://openreview.net/attachment?id=TnLFRhLuZ6&name=pdf">
              COMPOSE YOUR POLICIES! IMPROVING DIFFUSION-BASED OR FLOW-BASED ROBOT POLICIES VIA TEST-TIME DISTRIBUTION-LEVEL COMPOSITION
            </a><br>
            <em>TL;DR:</em> Introduces a method to compose flow/diffusion-based VLA policies at test time to improve performance over individual policies.
            The authors use convex optimization and test-time search to compose scores from multiple policies to improve performance.
          </li>
          <li>
            <a href="https://openreview.net/attachment?id=tc2UsBeODW&name=pdf">
              VLM4VLA: REVISITING VISION-LANGUAGE-MODELS IN VISION-LANGUAGE-ACTION MODELS
            </a><br>
            <em>TL;DR:</em> Compares a lot of VLMs as backbone choice for VLAs and finds that downstream performance has no correlation with VLM performance on standard benchmarks.
            This confirms my own experience in experimenting with various VLM backbones. However, the paper is still limited to benchmark setups only and does not test real robot results.
          </li>
        </ul>
          
        <h2 id="frontier-gap">10. The Hidden Gap Between Frontier and Research VLAs</h2>
        <figure class="figure-container">
          <img src="images/closed_vs_open_VLAs.png" alt="VLAs Overview">
          <figcaption>Figure 8: Overview of closed-weight vs open-weight VLAs.</a>.</figcaption>
        </figure>
        <p>
          On paper (pun intended:)), the gap seems small: on simulation setups (LIBERO, CALVIN) open-source VLAs surpass popular frontier baselines like Pi0.5. 
          In practice, there is still a bigger gap that only shows up precisely where current papers rarely evaluate: <em>zero-shot, open-world behavior after pretraining</em>. 
          Two weeks ago at CoRL, the Gemini-Robotics VLA demo attempted a wide range of novel tasks with arbitrary objects and paraphrased language. 
          My own VLA  <a href="https://arxiv.org/pdf/2509.04996">FLOWER</a> is state-of-the-art on CALVIN benchmark, but nowhere close to that level of zero-shot robustness. 
          Simulation Benchmarks hide this delta and current simulation setups don't optimize for this objective.<br><br>
          
          This gap is not unique to VLAs, the same is true for LLMs and VLMs.
          For VLAs, fully open-weight models (that share training recipe, data, code and weights) still lag significantly behind closed-weight models like Gemini-Robotics and Pi0.5 on zero-shot tasks.
          (although there seem to be some promising signs that this gap is closing with <a href="https://arxiv.org/abs/2508.21112">EO-1</a> but I haven't tried them myself yet).
          This is not to say that open-weight VLAs are useless, they are very useful for research and have strong performance in a lot of scenarios.
          But I wanted to highlight this gap, as I believe it is a significant issue for our community that needs to be addressed in the future.<br><br>
        </p>
      <p><strong>Why the gap exists (based on what’s visible from papers, discussion with peers and personal experience):</strong></p>
      <ul>
        <li><em>Benchmark progress saturation masks real progress.</em> When scores cluster near the ceiling, “+0.5%” is not evidence of real improvements.</li>
        <li><em>High quality Data Gap</em> The current available open-source data is limited in diversity and scale, which limits the training of more general models.</li>
        <li><em>Understanding of High Quality Data</em>It's not only the data scale its also about the missing knowledge gap of understanding high quality demonstration data.</li>
        <li><em>Evaluation scope is narrow.</em> Most papers report sim-only or small, locally finetuned setups. 
          Free-form zero-shot language following and unseen objects/rooms are very rare. 
          However, it's important to highlight that proper large-scale evals are only doable by companies.</li>
        <li><em>Operational constraints.</em> Research groups lack the manpower/time to run large, diverse real-world trials necessary for rapid iteration. 
          Frontier labs operate at a different scale of manpower, funding, and robot fleet size.
        Thus, many PhDs, including myself, iterate on simulation benchmarks to test and try out their ideas.</li>
        <li><em>Missing Peer Review Incentives.</em> Reviewers at most major conferences expect head-to-head comparison against open-weight but closed-source trained VLAs on standard sims and strong local finetuning numbers. 
          These benchmarks are good for paper acceptance, but weakly correlated with open-world performance.</li>
      </ul>
      <p><strong>Counterarguments against this thesis:</strong></p>
      <ul>
        <li><em>Research Findings are not equal to performance gains.</em>
          Many great papers I discussed here have general interesting findings useful for the community and research. 
          Zero-shot Performance is only one aspect of VLA Performance.</li>
      </ul>
      If you have good arguments for or against this thesis, please reach out to me, very happy to hear other opinions.<br><br>
      <figure class="figure-container">
        <img src="images/roboarena_leaderboard.png" alt="Zero-Shot Gap Illustration">
        <figcaption>Figure 9: Overview of the current <a href="https://robo-arena.github.io/" target="_blank">RoboArena</a> Leaderboard. There is only one non-Pi model that is near to being competitive in zero-shot tasks.</figcaption>
      </figure>
      <p><strong>What would help to bridge this gap without blowing the compute and human manpower budget:</strong></p>
      <ul>
        <li><em>Usage of public zero-shot fair benchmarks.</em> Track progress on setups like <a href="https://robo-arena.github.io/" target="_blank">RoboArena</a> that test post-pretraining generalization by independent operators. 
          Today, non-Pi models are scarcely represented and far behind. However, the training code for the higher ranked policies are all fully open-sourced by PI in this <a href="https://github.com/Physical-Intelligence/openpi" target="_blank">codebase</a> to facilitate further research.</li> 
          Another new interesting attempt to tackle this is <a href="https://manipulation-net.org/" target="_blank">ManipulationNet</a>. I hope the community is able to adopt some of these for better evaluation. 
          <li><em>Better Pretraining Recipes.”</em> I've seen only very few papers like <a href="https://openreview.net/attachment?id=kt51kZH4aG&name=pdf">X-VLA</a> that properly ablate all their pretraining design decisions to understand the impact of these for performance. 
        We need more papers like this that share full pretraining recipes and also discuss failed ideas that did not work during pretraining.</li>
        </ul>
      <p>
        I want to clearly highlight, that I don't think simulation or local finetuning are useless for research in contrast they are very important for many parts of robot learning. 
        Only that they are a poor proxy for the thing for the main argument of VLA: <em>robust zero-shot behavior in messy, new environments</em>. 
      </p>

        
        <h2 id="summary-outlook">11. Summary and Outlook</h2>

        <p>
          Overall, I am very positive about the current state of VLA research and the progress being made in this field.
          The trends above show strong interest and contributions across VLA models—from architecture design to training strategies and evaluation methods.
          However, there are also a few disappointing aspects aside from the zero-shot performance gap of the current state of VLA research that are worth highlighting.
        </p>

        <p><strong>Two Underrepresented Problems in Current VLA Research:</strong></p>

        <ul>
          <li><strong>Data quality:</strong> Despite being critical for VLA performance, surprisingly few ICLR 2026 submissions focused on data collection and curation. 
            It's an open secret that OXE is mostly low-quality data, yet we still lack good methods to quantify data quality in imitation learning. 
            Data-centric research is hard, but understanding how to curate high-quality datasets remains one of the most crucial unsolved problems in VLA research.</li>
          
          <li><strong>In-context learning:</strong> Given the success of in-context learning for LLMs and VLMs, I expected more VLA work in this direction but found almost none. 
            Language alone provides limited context for complex physical tasks—in-context learning could be key to enabling better prompting and zero-shot task generalization for VLAs. 
            There have been some good attempts, but it's still unclear how to best implement this for VLAs in a way that captures the rich contextual information needed to solve complex manipulation tasks.</li>
        </ul>

        <p>
          Despite these gaps, I remain optimistic that the field will continue to grow and evolve rapidly. 
          The explosive growth in submissions and the convergence on promising directions like discrete diffusion and embodied reasoning suggest that VLA research is maturing quickly. 
          As we address these fundamental challenges around data quality and contextual learning, we'll move closer to VLAs that can truly generalize in the messy, unstructured environments where robots need to operate.
        </p>
        
        <h2 id="cite">Cite this post</h2>
        <p>If you’d like to cite this article, use the BibTeX below:</p>
        <pre><code>@misc{reuss2025state-vla-iclr26,
          title        = {State of VLA Research at ICLR 2026},
          author       = {Reuss, Moritz},
          year         = {2025},
          month        = {October},
          howpublished = {\url{https://mbreuss.github.io/blog_post_iclr_26_vla.html}},
          note         = {Blog post},
        }</code></pre>

        
        <hr>
        <p style="text-align:center;">
          <a href="index.html">← Back to main page</a>
        </p>

      </td>
    </tr>
  </table>
</body>
</html>
