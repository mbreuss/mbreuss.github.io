<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>State of VLA Research at ICLR 2026 – Moritz Reuss</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">

  <!-- Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-ETMW7BYE0C"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-ETMW7BYE0C');
  </script>
</head>

<body>
  <table style="width:100%; max-width:800px; margin:auto;">
    <tr>
      <td>

        <h1 style="text-align:center;">State of VLA Research at ICLR 2026</h1>
        <p style="text-align:center; color:gray;">October 2025 • by Moritz Reuss</p>
        <hr>

        <p>
          Every year in autumn, the deadline for ICLR submissions looms over the machine learning community.
          Unlike NeurIPS or ICML, ICLR publicly releases all submissions a few weeks after the deadline, 
          giving a unique real-time snapshot of ongoing research around the world without the half-year delay.  
          Given my personal research interests, I looked into <strong>Vision-Language-Action (VLA) Models</strong> for robot learning by 
          scanning the submissions for key developments in that subfield to discuss interesting papers and trends.
          Below is a summary of general research trends with some highlighted papers for each category that I found interesting from this year's submission.
          Please note that this is only a personal selection of papers and I might have missed many interesting works.
          If you find any noteworthy papers that I missed, please let me know.

          Some general observations based on keyword search: last year there were 6 accepted papers and 3 rejected ones with "Vision-Language-Action" appearing in the keyword search on OpenReview.
          In 2024 it was only 1 rejected submission. This year, OpenReview shows 164 submissions with the same keyword. 
          Given the exponential growth, I’m excited to review the 2,100 VLA ICLR submissions next year.
        </p>

        <h2>Summary of Key Trends</h2>
        <p>By scanning and skim-reading most of the ICLR 2026 submissions with the VLA keyword, I identified the following key trends in VLA research with a lot of overlap between categories:</p>
        <ul>
          <li><a href="#discrete-diffusion">Discrete Diffusion VLAs</a></li>
          <li><a href="#embodied-cot">Embodied Chain-of-Thought</a></li>
          <li><a href="#tokenizers">New Discrete Tokenizers</a></li>
          <li><a href="#efficient-vla">Efficient VLAs</a></li>
          <li><a href="#vla-video">VLA + Video Prediction</a></li>
          <li><a href="#human-videos">Learning from Human Videos</a></li>
          <li><a href="#evaluation">Evaluation and Benchmarking of VLAs</a></li>
          <li><a href="#cross-action-space">Cross-Action-Space Learning</a></li>
          <li><a href="#other">Other Interesting Papers</a></li>
        </ul>

        <h2>Practitioners Guide to Understanding Benchmark Results in VLA Research</h2>
        <p>
          To better understand the results of the papers mentioned below, it’s important to have context on current VLA benchmarks.
          There is no clear statement possible to say which model is the best, as most papers only compare on standard simulation benchmarks.
          90% of papers mentioned here test on either 
          <a href="https://arxiv.org/abs/2306.03310" target="_blank">LIBERO</a>, 
          <a href="https://openreview.net/pdf?id=LZh48DTg71" target="_blank">SIMPLER</a>, or 
          <a href="https://arxiv.org/abs/2112.03227" target="_blank">CALVIN</a>.
          LIBERO is basically solved and showing 99% vs 98% is not very helpful — you don’t need VLAs or large-scale pretraining to be competitive.
          All discrete diffusion policies below test on LIBERO and achieve between 95–98% average over the 4 versions (Goal, Spatial, Long, Object). 
          Based on these results, it’s impossible to say which model is better since they are close to the ceiling anyway.
          A properly tuned Diffusion Policy can get there without VLAs, even though most cited DP baselines perform worse. 
          CALVIN is also almost solved with models like <a href="https://arxiv.org/abs/2509.04996" target="_blank">FLOWER</a>.
          For CALVIN, a score >4 on ABC is standard; >4.5 is SOTA. For ABCD, >4.6 is strong. 
          SIMPLER is less consistent (40–90%), and for the Google Robot benchmark, SOTA models achieve ~70%. 
          RLBench (<a href="https://arxiv.org/abs/1909.12271" target="_blank">RLBench</a>) has also gained popularity, though VLAs still trail behind specialized 3D methods like 
          <a href="https://arxiv.org/abs/2402.10885" target="_blank">3DDA</a>.
          Real-world results remain rare but highly valuable.
        </p>

        <hr>

        <h2 id="discrete-diffusion">1. Discrete Diffusion VLAs</h2>
        <p>
          Given the success of discrete diffusion models in other domains like text 
          (<a href="https://arxiv.org/abs/2209.09402" target="_blank">D3PM</a>) 
          and vision-language models 
          (<a href="https://arxiv.org/abs/2505.16933" target="_blank">LLaDA-V</a>), 
          it’s no surprise that this trend is also entering VLA research. 
          Discrete diffusion allows sequence generation in parallel, ideal for discrete action tokens.
          Combined with Embodied Chain-of-Thought (ECoT), this enables generating sub-goals and reasoning jointly with actions, addressing slow autoregressive limitations.
        </p>

        <p><strong>Notable papers:</strong></p>
        <ul>
          <li><a href="https://openreview.net/attachment?id=YWeNCMxdhM&name=pdf" target="_blank">DISCRETE DIFFUSION VLA</a>: Discrete diffusion decoding for parallelized action generation; adaptive decoding for inference; strong results on LIBERO + SIMPLER.</li>
          <li><a href="https://openreview.net/attachment?id=2rxgospB5s&name=pdf" target="_blank">dVLA: DIFFUSION VISION-LANGUAGE-ACTION MODEL WITH MULTIMODAL CHAIN-OF-THOUGHT</a>: Combines ECoT with Discrete Diffusion for fast co-generation of text and actions.</li>
          <li><a href="https://openreview.net/attachment?id=mNya9d1DA2&name=pdf" target="_blank">DIVA</a>: Focuses on token substitution during inference for better discrete diffusion performance.</li>
          <li><a href="https://openreview.net/attachment?id=UvQOcw2oCD&name=pdf" target="_blank">UNIFIED DIFFUSION VLA</a>: Joint discrete denoising of future frames and actions with causal masking; strong CALVIN + LIBERO results.</li>
        </ul>

        <h2 id="embodied-cot">2. Embodied Chain-of-Thought (ECoT)</h2>
        <p>
          Since <a href="https://arxiv.org/abs/2407.08693" target="_blank">the first ECoT paper</a> (CoRL 2024), there’s growing interest in combining spatial reasoning with action prediction.
          Analyses like <a href="https://arxiv.org/abs/2505.08243" target="_blank">ECoT Training Analysis</a> show these objectives bridge the gap between VLM pretraining and embodied control.
          However, autoregressive tokenization limits efficiency, inspiring discrete diffusion extensions and hybrid training strategies.
        </p>
        <p><strong>Notable papers:</strong></p>
        <ul>
          <li><a href="https://openreview.net/attachment?id=sFO9d6XSlf&name=pdf" target="_blank">ACTIONS AS LANGUAGE</a>: Fine-tunes VLMs into VLAs without catastrophic forgetting by relabeling actions as text subtasks; bridges domain gap via LoRA finetuning.</li>
          <li><a href="https://openreview.net/attachment?id=IBJtOltTbx&name=pdf" target="_blank">HYBRID TRAINING FOR VLAs</a>: Decomposes ECoT into think-act-follow subtasks for faster inference while preserving reasoning gains.</li>
        </ul>

        <h2 id="tokenizers">3. New Tokenizers</h2>
        <p>
          Discrete Action Tokenizers for VLAs emerged with 
          <a href="https://arxiv.org/abs/2501.09747" target="_blank">FAST</a>, 
          showing discrete tokenization can bridge VLMs and action learning without flow/diffusion models.
          This year, multiple tokenizers build on FAST using 
          Residual Vector Quantization (RVQ) 
          (<a href="https://arxiv.org/abs/2107.03312" target="_blank">SoundStream</a>) 
          and B-Splines from 
          <a href="https://arxiv.org/abs/2506.06072" target="_blank">BEAST</a>.
        </p>

        <p><strong>Notable papers:</strong></p>
        <ul>
          <li><a href="https://openreview.net/attachment?id=k6nTUFoqeT&name=pdf" target="_blank">FASTer</a>: Combines RVQ with frequency and time-domain losses using DCT; patchifies action tokens temporally and across grouped dimensions; higher compression ratio than FAST.</li>
          <li><a href="https://openreview.net/attachment?id=CuzTXLB7Jz&name=pdf" target="_blank">OMNISAT</a>: Uses B-Spline based encoding (inspired by <a href="https://arxiv.org/abs/2506.06072" target="_blank">BEAST</a>) with a two-stage normalization + VQ-VAE process; outperforms FAST and BEAST across LIBERO and SIMPLER.</li>
        </ul>

        <h2 id="efficient-vla">4. Efficient VLAs</h2>
        <p>
          Efficiency remains key for researchers with limited compute. Methods focus on smaller models, efficient tokenizers, and faster inference via quantization or distillation.
          Making VLAs more accessible will democratize embodied AI research.
        </p>

        <h2 id="vla-video">5. VLA + Video Prediction</h2>
        <p>
          Following <a href="https://arxiv.org/abs/2312.13139" target="_blank">GR-1</a>, video-based policies are rising. These models often fine-tune pretrained video or image diffusion models to predict future frames and actions jointly.
          Video generation captures physics and grounding priors beneficial for control, though expensive to train.
        </p>

        <p><strong>Notable papers:</strong></p>
        <ul>
          <li><a href="https://openreview.net/attachment?id=DdrsHWobR1&name=pdf" target="_blank">DISENTANGLED ROBOT LEARNING</a>: Pretrains separate forward/inverse dynamics models and combines them during finetuning; strong CALVIN results.</li>
          <li><a href="https://openreview.net/attachment?id=wPEIStHxYH&name=pdf" target="_blank">COSMOS POLICY</a>: Fine-tunes NVIDIA’s Cosmos Video Foundation Model for action prediction; injects action/value modalities; competitive LIBERO results with real-world validation.</li>
        </ul>

        <h2 id="evaluation">6. Evaluation and Benchmarking</h2>
        <p>
          Benchmark saturation makes meaningful comparison difficult.
          New simulation environments, Real2Sim setups, and generative evaluation frameworks are emerging.
        </p>

        <h2 id="cross-action-space">7. Cross-Action-Space Learning</h2>
        <p>
          Pretraining across diverse action spaces remains challenging but crucial for generalist VLAs. 
          Datasets like <a href="https://arxiv.org/abs/2505.11709" target="_blank">EgoDex</a> enable progress.
          DeepMind’s <a href="https://arxiv.org/abs/2510.03342" target="_blank">Gemini Robotics 1.5</a> hints that large-scale data and model size can solve cross-embodiment transfer.
        </p>

        <p><strong>Notable papers:</strong></p>
        <ul>
          <li><a href="https://openreview.net/attachment?id=kt51kZH4aG&name=pdf" target="_blank">X-VLA</a>: Uses soft-prompt tokens for scalable cross-embodiment learning; strong results on LIBERO, CALVIN, SIMPLER, and VLABench.</li>
          <li><a href="https://openreview.net/attachment?id=TX3oGD99CJ&name=pdf" target="_blank">HIMOE-VLA</a>: Introduces Hierarchical Mixture-of-Experts Transformers with action-space and heterogeneity-balancing MoEs.</li>
        </ul>

        <h2 id="other">8. Other Interesting Papers</h2>
        <p><a href="https://openreview.net/attachment?id=tc2UsBeODW&name=pdf" target="_blank">VLM4VLA</a>: Revisits VLM backbone choice for VLAs; finds no correlation between upstream VLM benchmarks and downstream control performance.</p>

        <h2 id="summary-outlook">9. Summary and Outlook</h2>
        <p>
          Overall, VLA research is accelerating rapidly.  
          From discrete diffusion and action tokenization to efficient architectures and video-based reasoning, the field is maturing fast.
          Expect 2026 to bring large-scale generalist policies capable of reasoning, perception, and control across multiple embodiments.
        </p>

        <hr>
        <h2 id="references">References</h2>
        <ol>
          <li><a href="https://arxiv.org/abs/2509.04996" target="_blank">FLOWER: Unified Vision-Language-Action Model via Intermediate-Modality Fusion</a>, ICLR 2026.</li>
          <li><a href="https://arxiv.org/abs/2506.06072" target="_blank">BEAST: B-Spline Tokenization for Continuous Robot Actions</a>, ICLR 2026.</li>
          <li><a href="https://arxiv.org/abs/2501.09747" target="_blank">FAST: Frequency-Aware Action Space Tokenizer</a>, ICLR 2025.</li>
          <li><a href="https://arxiv.org/abs/2306.03310" target="_blank">LIBERO: Benchmarking Generalization for Robot Learning</a>, CoRL 2023.</li>
          <li><a href="https://arxiv.org/abs/2112.03227" target="_blank">CALVIN: Visual Language Conditioning for Manipulation</a>, CoRL 2021.</li>
          <li><a href="https://arxiv.org/abs/2403.03181" target="_blank">VQ-VAE for Robot Actions</a>, ICRA 2024.</li>
          <li><a href="https://arxiv.org/abs/2312.13139" target="_blank">GR-1: General Robot Policy via Video Prediction</a>, ICLR 2024.</li>
          <li><a href="https://arxiv.org/abs/2407.08693" target="_blank">Embodied Chain-of-Thought (ECoT)</a>, CoRL 2024.</li>
          <li><a href="https://arxiv.org/abs/2505.08243" target="_blank">Analyzing ECoT Training</a>, ICLR 2026.</li>
          <li><a href="https://arxiv.org/abs/2505.11709" target="_blank">EgoDex: Egocentric Human Action Dataset</a>, ICLR 2026.</li>
          <li><a href="https://arxiv.org/abs/2510.03342" target="_blank">Gemini Robotics 1.5</a>, DeepMind Technical Report 2025.</li>
          <li><a href="https://arxiv.org/abs/2402.10885" target="_blank">3DDA: 3D Diffusion for Manipulation</a>, CVPR 2024.</li>
          <li><a href="https://arxiv.org/abs/2107.03312" target="_blank">SoundStream: End-to-End Neural Audio Codec with RVQ</a>, NeurIPS 2021.</li>
          <li><a href="https://openreview.net/pdf?id=LZh48DTg71" target="_blank">SIMPLER Benchmark</a>, ICLR 2024.</li>
        </ol>

        <hr>
        <p style="text-align:center;">
          <a href="index.html">← Back to main page</a>
        </p>

      </td>
    </tr>
  </table>
</body>
</html>
