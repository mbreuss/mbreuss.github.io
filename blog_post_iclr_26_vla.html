<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- SEO Meta Tags -->
  <title>State of Vision-Language-Action (VLA) Research at ICLR 2026 – Moritz Reuss</title>
  <meta name="description" content="Comprehensive analysis of 164 Vision-Language-Action (VLA) model submissions at ICLR 2026. Discover trends in discrete diffusion VLAs, reasoning models, benchmarks (LIBERO, CALVIN, SIMPLER), and the gap between frontier and academic research.">
  <meta name="keywords" content="VLA models, Vision-Language-Action, ICLR 2026, robot learning, embodied AI, discrete diffusion VLA, VLA benchmarks, LIBERO, CALVIN, SIMPLER, robotics research">
  <meta name="author" content="Moritz Reuss">
  <meta name="robots" content="index, follow">
  <link rel="canonical" href="https://mbreuss.github.io/blog_post_iclr_26_vla.html">
  
  <!-- Open Graph / Social Media Meta Tags -->
  <meta property="og:title" content="State of VLA Research at ICLR 2026">
  <meta property="og:description" content="Analysis of 164 Vision-Language-Action model submissions at ICLR 2026. Learn about discrete diffusion VLAs, reasoning models, and current benchmark trends.">
  <meta property="og:type" content="article">
  <meta property="og:url" content="https://mbreuss.github.io/blog_post_iclr_26_vla.html">
  <meta property="og:image" content="https://mbreuss.github.io/images/blog_header.png">
  <meta property="article:published_time" content="2025-10-10">
  <meta property="article:author" content="Moritz Reuss">
  
  <!-- Twitter Card Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="State of VLA Research at ICLR 2026">
  <meta name="twitter:description" content="Comprehensive analysis of Vision-Language-Action models at ICLR 2026 - discrete diffusion, reasoning VLAs, and benchmark insights.">
  <meta name="twitter:image" content="https://mbreuss.github.io/images/blog_header.png">
  
  <!-- Schema.org Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "headline": "State of Vision-Language-Action (VLA) Research at ICLR 2026",
    "author": {
      "@type": "Person",
      "name": "Moritz Reuss"
    },
    "datePublished": "2025-10-10",
    "dateModified": "2025-10-10",
    "description": "Comprehensive analysis of Vision-Language-Action models at ICLR 2026, covering discrete diffusion, reasoning VLAs, benchmarks, and research trends.",
    "keywords": "VLA models, Vision-Language-Action, robot learning, ICLR 2026, embodied AI, discrete diffusion",
    "image": "https://mbreuss.github.io/images/blog_header.png",
    "url": "https://mbreuss.github.io/blog_post_iclr_26_vla.html"
  }
  </script>
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  
  <style>
    .figure-container {
      max-width: 800px;
      margin: 0 auto;
    }
    .figure-container img {
      width: 100%;
      height: auto;
    }
  </style>

  <!-- Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-ETMW7BYE0C"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-ETMW7BYE0C');
  </script>
</head>

<body>
  <table style="width:100%; max-width:800px; margin:auto;">
    <tr>
      <td>

        <h1 style="text-align:center;">State of VLA Research at ICLR 2026</h1>
        <p style="text-align:center; color:gray;">October 2025 • by Moritz Reuss</p>
        <hr>
        <figure class="figure-container">
          <img src="images/blog_header.png" alt="Vision-Language-Action (VLA) models research overview and trends at ICLR 2026">
        </figure>
        <p>
          Every year in autumn, the ICLR submission deadline looms over the machine learning community.
          Unlike NeurIPS or ICML, ICLR publicly releases all anonymous submissions a few weeks after the deadline, 
          providing a unique real-time snapshot of ongoing research around the world without the typical six-month delay of other top ML conferences.  
          Given my personal research interests, I wanted to analyze <strong>Vision-Language-Action (VLA) Models</strong> research and share insights from this year's submissions.<br><br>

          In this blog post, I share my findings about current trends and challenges in VLA research, highlighting the some interesting papers submitted to ICLR 2026.
          To help researchers and practitioners better understand the current state of VLA research, I've also included a practical guide on how to interpret benchmark results in VLA papers.
          Finally, there is a discussion about the hidden gap between frontier labs and academic research labs in the VLA field—a gap that isn't visible from reading papers alone.<br><br>

          Below is a summary of general research trends with highlighted papers for each category that I found interesting from this year's submissions.
          Please note that this represents only my personal selection and opinion of papers and research, and I may have missed other excellent work.
          If you know of noteworthy papers that I missed, please let me know.<br><br>

          <strong>The Explosive Growth of VLA Research:</strong> Some remarkable observations about VLA research growth based on keyword searches on OpenReview: 
          In 2024, there was only 1 rejected submission with "Vision-Language-Action" appearing in the keyword search.
          In 2025, there were 6 accepted papers and 3 rejected submissions with the same keyword.
          For 2026, OpenReview shows 164 submissions with the same keyword—a large increase in just one year! 
          Given this exponential growth trajectory, I'm both excited and slightly terrified to review the projected 2,100+ VLA submissions at ICLR 2027.
        </p>

        <h2>Table of Contents</h2>
        <nav style="background: #f8f9fa; padding: 20px; border-left: 4px solid #4CAF50; margin: 20px 0;">
          <ol style="line-height: 1.8;">
            <li><a href="#key-trends">Summary of Key Trends</a></li>
            <li><a href="#practitioners-guide">Practitioners Guide to Understanding Benchmark Results</a></li>
            <li><a href="#discrete-diffusion">Discrete Diffusion VLAs</a></li>
            <li><a href="#embodied-cot">Reasoning VLAs and Embodied Chain-of-Thought (ECoT)</a></li>
            <li><a href="#tokenizers">New Discrete Tokenizers</a></li>
            <li><a href="#efficient-vla">Efficient VLAs</a></li>
            <li><a href="#RL-vla">RL for VLAs</a></li>
            <li><a href="#vla-video">VLA + Video Prediction</a></li>
            <li><a href="#evaluation">Evaluation and Benchmarking of VLAs</a></li>
            <li><a href="#cross-action-space">Cross-Action-Space Learning</a></li>
            <li><a href="#other">Other Interesting Papers</a></li>
            <li><a href="#frontier-gap">The Hidden Gap Between Frontier and Research VLAs</a></li>
            <li><a href="#summary-outlook">Summary and Outlook</a></li>
          </ol>
        </nav>


        <h2 id="key-trends">Summary of Key Trends</h2>
        <p>By scanning and reading most of the ICLR 2026 submissions with the VLA keyword, I identified the following key trends in VLA research with a lot of overlap in between these categories:</p>
        <ul>
          <li><a href="#discrete-diffusion">Discrete Diffusion VLAs</a></li>
          <li><a href="#embodied-cot">Reasoning VLAs and Embodied Chain-of-Thought</a></li>
          <li><a href="#tokenizers">New Discrete Tokenizers</a></li>
          <li><a href="#efficient-vla">Efficient VLAs</a></li>
          <li><a href="#RL-vla">RL for VLAs</a></li>
          <li><a href="#vla-video">VLA + Video Prediction</a></li>
          <li><a href="#evaluation">Evaluation and Benchmarking of VLAs</a></li>
          <li><a href="#cross-action-space">Cross-Action-Space Learning</a></li>
          <li><a href="#other">Other Interesting Papers</a></li>
        </ul>

        <h2 id="practitioners-guide">Practitioners Guide to Understanding Benchmark Results in VLA Research</h2>
        <figure class="figure-container">
          <img src="images/vla_sim_setups.png" alt="VLA Simulation Setups Overview">
          <figcaption>Figure 2: Overview of the most popular VLA simulation benchmarks: <a href="https://arxiv.org/abs/2306.03310" target="_blank">LIBERO</a>, <a href="https://openreview.net/pdf?id=LZh48DTg71" target="_blank">SIMPLER</a> and <a href="https://arxiv.org/abs/2112.03227" target="_blank">CALVIN</a> 
          </figcaption>
        </figure>

        <p>
          Before diving into the research trends, I want to provide a quick guide for practitioners on how to interpret benchmark results in VLA papers.
          So you are reading a new VLA paper and want to understand if the claimed results are actually good or not?
          In order to better understand the VLA results, it is important to have some context on the current state of popular VLA benchmarks and what good performance looks like<br><br>.

          Given the current benchmark there is no clear statement possible to say which model is the best, as most papers only compare against each other on the usual simulation benchmarks.
          90% of papers mentioned in this post all test in either <a href="https://arxiv.org/abs/2306.03310" target="_blank">LIBERO</a>, <a href="https://openreview.net/pdf?id=LZh48DTg71" target="_blank">SIMPLER</a> or <a href="https://arxiv.org/abs/2112.03227" target="_blank">CALVIN</a>.<br><br>
          LIBERO is basically solved and showing 99% vs 98% is not very helpful and you don't need VLAs and large-scale pretraining to get competitive results.
          E.g. all mentioned concurrent discrete diffusion policies below test on LIBERO and all achieve between 95-98% average over the 4 versions (Goal, Spatial, Long, Object).
          It's also funny to note that LIBERO was designed as a life-long learning benchmark but 99% of models reporting results on it just train on the full dataset and don't do any continual learning.
          Based on these results it is impossible to say which model is better, as they are all very close to the ceiling anyways and you don't need internet scale pretraining to solve these.<br><br>

          Rule of thumb for getting a impression a VLA is decent or near sota levels on these: LIBERO: >95% is expected for Spatial, Goal and Object, 90-95% is required for Long and 90, lower than 90% is only decent for non-wrist came or few-shot stuff.
          A properly tuned Diffusion Policy can get you there without VLAs although the most cited DP baselines is showing worse results. 
          CALVIN is also almost saturated by current sota models like <a href="https://arxiv.org/pdf/2509.04996">FLOWER</a>.
          For CALVIN higher than 4 score for ABC is standard now and above 4.5 is sota regime. 
          For the D version 3.75 is standard and above 4 is very good. 
          For the ABCD version results above 4.6 are relevant.<br><br>

          SIMPLER is kind of in a weird spot and I don't know if progress there translates to real world. The bridge version ranges from 40% up to 99% success rate, making it hard to judge. 
          For the Google Robot version current sota models achieve around 70%-80% success rate. 
          Somehow, <a href="https://arxiv.org/abs/1909.12271" target="_blank">RLBench</a> (the most popular 3d policy benchmark) has gained more popularity in VLA benchmarking, but all VLAs are still far away from 3D SOTA methods like <a href="https://arxiv.org/abs/2402.10885" target="_blank">3DDA</a>. 
          Most VLA policies try to avoid comparing against all relevant 3D baselines for some reason. 
          Any real world results are very important and more is better, as sim-only is hard to trust especially with VLA papers that use models with 7B+ parameters and are very good in overfitting successfully on these benchmarks.<br><br>
        </p>

        <hr>

        <h2 id="discrete-diffusion">1. Discrete Diffusion VLAs</h2>
        <figure class="figure-container">
          <img src="images/discrete_diff_comp.png" alt="Discrete Diffusion VLA Architecture">
          <figcaption>Figure 1: Overview of discrete diffusion paradigm for action generation. Image from <a href="https://openreview.net/attachment?id=YWeNCMxdhM&name=pdf">DISCRETE DIFFUSION VLA</a>.</figcaption>
        </figure>
        <p>
          Given the success of discrete diffusion models in other domains like text (e.g. <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/eb0b13cc515724ab8015bc978fdde0ad-Paper-Conference.pdf">(MDLM</a>) and VLMs (e.g. <a href="https://arxiv.org/pdf/2505.16933">LLaDA-V</a>), it is no surprise that this trend is also making its way into VLA research. 
          Why discrete diffusion? Compared to autoregressive models, diffusion models can generate sequences in parallel, which is a big advantage for discrete action token generation. Instead of having to run your policy 100 times, you can generate long action sequences in a few forward passes. 
          In addition, you can combine it with ideas from Embodied Chain-of-Thought (see next section) to generate sub-goals and reasoning together with actions in parallel. 
          This tackles some of the biggest limitations of ECoT from prior work, which was extremely slow due to the autoregressive nature of the VLM models.
          Current attempts for discrete Diffusion either finetune an autoregressive VLM with discrete diffusion because the variety of discrete diffusion VLMs is very limited.
          Other work use LLaDA-V as a pretrained backbone with good success. 
        </p>

        <p><strong>Notable papers:</strong></p>
        <ul>
          <li>
            <a href="https://openreview.net/attachment?id=YWeNCMxdhM&name=pdf">
              DISCRETE DIFFUSION VLA: BRINGING DISCRETE DIFFUSION TO ACTION DECODING IN VISION-LANGUAGE-ACTION POLICIES
            </a><br>
            <em>TL;DR:</em> Take OpenVLA and apply Discrete Diffusion Action Prediction for fast action chunk-based generation of discrete action tokens. Also proposes adaptive decoding for inference.
            Strong results on LIBERO + SIMPLER.
          </li>
          <li>
            <a href="https://openreview.net/attachment?id=2rxgospB5s&name=pdf">
              dVLA: DIFFUSION VISION-LANGUAGE-ACTION MODEL WITH MULTIMODAL CHAIN-OF-THOUGHT
            </a><br>
            <em>TL;DR:</em> Another Discrete Diffusion VLA using Co-Generation for Future Frames and text + actions given the advantage of fast parallel sampling of Discrete Diffusion over AR models. 
            Basically ECoT + Discrete Diffusion done well. 
            Also good results in LIBERO + real world experiments.
          </li>
          <li>
            <a href="https://openreview.net/attachment?id=mNya9d1DA2&name=pdf">
              DIVA: DISCRETE DIFFUSION VISION-LANGUAGE-ACTION MODELS FOR PARALLELIZED ACTION GENERATION
            </a><br>
            <em>TL;DR:</em> Another discrete Diffusion VLA that also focuses on how to substitute tokens during inference for better performance.
          </li>
          <li>
            <a href="https://openreview.net/attachment?id=UvQOcw2oCD&name=pdf">
              UNIFIED DIFFUSION VLA: VISION-LANGUAGE-ACTION MODEL VIA JOINT DISCRETE DENOISING DIFFUSION PROCESS
            </a><br>
            <em>TL;DR:</em> Generates future frames and discrete actions together with block-wise causal masking. Results on CALVIN, LIBERO and SIMPLER are good.
          </li>
        </ul>

        <h2 id="embodied-cot">2. Reasoning VLAs and Embodied Chain-of-Thought (ECoT)</h2>
        <p>
          Since <a href="https://arxiv.org/abs/2407.08693" target="_blank">the first ECoT paper</a> (CoRL 2024), there has been a growing interest in combining spatially grounded reasoning with action prediction for improving VLAs.
          By combining subtask and bounding box prediction, the VLM gets better representations for embodied stuff and show improved performance on benchmarks.
          Prior results in analysing <a href="https://arxiv.org/abs/2505.08243" target="_blank">ECoT Training </a> show, that these objectives seem to help to bridge the representation gap between VLM pretraining and robotic related task. 
          However, the main limitation of prior ECoT work was the autoregressive nature of the VLAs and its increased token count which result slow in training and inference.
          Overall, it remains an open question how to best implement grounded reasoning for VLAs. 
          Most methods focussing on ECoT still use the same dataset but dont experiment a lot with which type of objectives are ideal.
        </p>

        <p><strong>Notable papers:</strong></p>
        <ul>
          <li>
            <a href="https://openreview.net/attachment?id=sFO9d6XSlf&name=pdf">
              ACTIONS AS LANGUAGE: FINE-TUNING VLMS INTO VLAS WITHOUT CATASTROPHIC FORGETTING
            </a><br>
            <em>TL;DR:</em> Instead of directly fine-tuning VLMs with discrete action tokens to become VLAs, which results in catastrophic forgetting, they relabel robot datasets with subtasks, actions as text and intermediate motion-planning like "move left".
            This training method is able to bridge the domain gap of VLMs without reducing performance on other VQA benchmarks from pretraining. 
            Finally cheap LORA finetuning is enough to get strong results for action prediction while maintaining VLM reasoning capabilities.
          </li>
          <li>
            <a href="https://openreview.net/attachment?id=IBJtOltTbx&name=pdf">
              HYBRID TRAINING FOR VISION-LANGUAGE-ACTION MODELS
            </a><br>
            <em>TL;DR:</em> Decomposes ECoT pretraining in several subtasks of think act and follow, that enable to maintain the performance benefits with fast inference. 
            Similar findings, that Co-Training with ECoT stuff results in better representations for action prediction.
          </li>
        </ul>

        <h2 id="tokenizers">3. New Tokenizers</h2>
        <figure class="figure-container">
          <img src="images/faster_tokenizer.png" alt="FASTer Tokenizer Overview">
          <figcaption>Figure 3: FASTer Tokenizer Overview. The tokenizer combines RVQ quantization with frequency and time domain losses. Image from <a href="https://openreview.net/attachment?id=k6nTUFoqeT&name=pdf">FASTer</a>.</figcaption>
        </figure>
        <p>
          Discrete Action Tokenizers for Vision-Language-Action Models is an open problem which was started by <a href="https://arxiv.org/abs/2501.09747" target="_blank">FAST</a>, showing the potential of discrete tokenizers for easy combination of VLMs with action learning without the more complicated flow/diffusion expert.
          Discrete tokenizers can directly be applied to pretrained VLMs without any architecture modifications. 
          Moreover, the domain gap between VLM pretraining and discrete action tokens is smaller compared to trying to force the VLM to predict velocity fields.
          Before <a href="https://arxiv.org/abs/2501.09747" target="_blank">FAST</a>, <a href="https://arxiv.org/pdf/2403.03181" target="_blank">VQ-VAEs</a> and discrete binning (used in <a href="https://arxiv.org/pdf/2212.06817" target="_blank">RT-1</a>) have been used, but they have several limitations like low compression ration and struggle to encode long-action chunks given the limited information content of tokens.
          This year there are several interesting new tokenizers that try to tackle these limitations with different approaches.
          New tokenizers combine ideas from Residual Vector Quantization (RVQ) (<a href="https://arxiv.org/abs/2107.03312" target="_blank">SoundStream</a>) to get a higher compression ratio, B-Splines ideas from <a href="https://arxiv.org/pdf/2506.06072" target="_blank">BEAST tokenizer</a> and DCT prediction losses inspired by <a href="https://arxiv.org/abs/2501.09747" target="_blank">FAST</a>.
          Very excited to give them a shot in my own research soon. 

        <p><strong>Notable papers:</strong></p>
        <ul>
          <li>
            <a href="https://openreview.net/attachment?id=k6nTUFoqeT&name=pdf">
              FASTER: TOWARD POWERFUL AND EFFICIENT AUTOREGRESSIVE VISION–LANGUAGE–ACTION MODELS WITH LEARNABLE ACTION TOKENIZER AND BLOCK-WISE DECODING
            </a><br>
            <em>TL;DR:</em> Introduces a novel discrete action tokenizer called FASTer, that combines Residual Vector Quantification (RVQ) with a frequency L1 loss using DCT and time domain L1 loss for improved performance. 
            Also patchifies action tokens along the temporal axis and grouped action dimension axis (e.g. base motion, arm joints). It has a higher compression ratio than <a href="https://arxiv.org/abs/2501.09747" target="_blank">FAST</a> and results on SIMPLER and LIBERO are strong.
          </li>
          <li>
            <a href="https://openreview.net/attachment?id=CuzTXLB7Jz&name=pdf">
              OMNISAT: COMPACT ACTION TOKEN, FASTER AUTOREGRESSION FOR VISION-LANGUAGE-ACTION MODELS
            </a><br>
            <em>TL;DR:</em> Another tokenizer for VLAs that uses our <a href="https://arxiv.org/pdf/2506.06072" target="_blank">BEAST</a> paper idea of B-Splines for compact representation of continuous action chunks.
            It uses a two stage encoding process: First, aligning the different action chunk lengths of different embodiments into a normalized, fixed-length representation.
            Next, it uses a B-Spline based encoder to get a compact representation of the normalized action chunk. Finally, a VQ-VAE is used to get discrete tokens.
            Results on LIBERO and SIMPLER are good and across all benchmarks improves upon both <a href="https://arxiv.org/abs/2501.09747" target="_blank">FAST</a> and <a href="https://arxiv.org/pdf/2506.06072" target="_blank">BEAST</a>.
          </li>
        </ul>

        <h2 id="efficient-vla">4. Efficient VLAs</h2>
        <p>As someone working on this topic myself, I understand the pain of trying to train and run larger VLAs on limited compute setups. 
          Thus, efficient VLAs are always relevant. Especially, since this gives labs with limited compute access the chance to work on VLAs too.
          There are several interesting papers this year that try to tackle this problem with different approaches.
          Generally, one can divide them into two categories: try to make training and models more efficient by making smaller VLAs or better tokenizers etc. 
          Otherwise, a lot of papers focus on making inference more efficient by using better quantization, distillation or similar ideas. 
        </p>

        <p><strong>Notable papers:</strong></p>
        <ul>
          <li>
            <a href="https://openreview.net/attachment?id=bsXkBTZjgY&name=pdf">
              HYPERVLA: EFFICIENT INFERENCE IN VISION- LANGUAGE-ACTION MODELS VIA HYPERNETWORKS
            </a><br>
            <em>TL;DR:</em> HyperVLA uses hypernetworks to generate small task-specific policies conditioned on language instructions and initial images, dramatically reducing inference cost while maintaining performance by only activating the compact generated policy during execution instead of a big VLA model.
          </li>

          <li>
            <a href="https://openreview.net/attachment?id=TpL2nXanru&name=pdf">
              AUTOQVLA: NOT ALL CHANNELS ARE EQUAL IN VISION-LANGUAGE-ACTION MODEL’S QUANTIZATION
            </a><br>
            <em>TL;DR:</em> Analysis quantization of OpenVLA and proposes improved quantization method for maintaining performance with only 30% of the original VRAM requirements.
          </li>
        </ul>

        <h2 id="efficient-vla">5. RL for VLAs</h2>
        <p>Finetuning VLAs to go from 70-80% success rate in the real world towards 99% is still an open problem.  
          There is a lot of hope for RL to close this gap. 
          While a lot of attempts have been made before, no method has established itself as the go-to method yet.
          This year there are several interesting papers that try to tackle this problem with different approaches.
        </p>

        <p><strong>Notable papers:</strong></p>
        <ul>
          <li>
            <a href="https://openreview.net/attachment?id=eUGoqrZ6Ea&name=pdf">
              SELF-IMPROVING VISION-LANGUAGE-ACTION MODELS WITH DATA GENERATION VIA RESIDUAL RL
            </a><br>
            <em>TL;DR:</em> Residual RL method that collects more data with frozen VLA and small residual policy. The residual interventions are used to get more high quality data with recovery behavior.
            Finally the VLA is finetuned using SFT. Results on LIBERO achieve 99%.
            </li>

          <li>
            <a href="https://openreview.net/attachment?id=TpL2nXanru&name=pdf">
              PROGRESSIVE STAGE-AWARE REINFORCEMENT FOR FINE-TUNING VISION-LANGUAGE-ACTION MODELS
            </a><br>
            <em>TL;DR:</em> The method breaks robot tasks into semantic stages (Reach→Grasp→Transport→Place) and assigns rewards to each stage instead of the whole trajectory. It uses STA-TPO for offline preference learning and STA-PPO for online reinforcement learning, both operating at the stage level.
            Results on Bridge SIMPLER of 98%. 
          </li>
        </ul>

        <h2 id="vla-video">6. VLA + Video Prediction</h2>
        <figure class="figure-container">
          <img src="images/video_policy_header.png" alt="Video Pretraining for VLAs">
          <figcaption>Figure 4: Example Pretraining Paradigm from <a href="https://openreview.net/attachment?id=DdrsHWobR1&name=pdf">DISENTANGLED ROBOT LEARNING Paper</a><br>
          </figcaption>
        </figure>
        <p>Started by strong results from <a href="https://arxiv.org/pdf/2312.13139" target="_blank">GR-1 paper</a> from ICLR 2024 that showed the potential of video-based policies, there is a growing interest in this subfield.
          These types of policies can be divided into two general categories: starting by a VLM, that optionally has been trained on image generation and continue training it with future frame and action prediction. 
          Otherwise several papers instead start from a Video Foundation Model and modify it to also generate actions. 
          Since most state-of-the-art video foundation models are diffusion/flow-based these policies typically struggle with slow inference speed. 
          Overall, the results demonstrate that video generation and the required physics understanding and language grounding are a very useful prior for robot learning.
          Compared to VLAs initialized from VLMs this subfield is way less popular and I hope to see more research in this direction.
          What holds back more research in this direction is the high requirements for finetuning sota video models like WAN even compared to finetuning VLM for VLAs.
        </p>

        <p><strong>Notable papers:</strong></p>
        <ul>
          <li>
            <a href="https://openreview.net/attachment?id=DdrsHWobR1&name=pdf">
              DISENTANGLED ROBOT LEARNING VIA SEPARATE FORWARD AND INVERSE DYNAMICS PRETRAINING
            </a><br>
            <em>TL;DR:</em> Introduces a novel approach to robot learning by pretraining separate forward and inverse dynamics models pretraining.
            In the second stage they are combined again for a coupled finetuning of the policy. Results on CALVIN are good on SIMPLER decent.
          </li>

          <li>
            <a href="https://openreview.net/attachment?id=wPEIStHxYH&name=pdf">
              COSMOS POLICY: FINE-TUNING VIDEO MODELS FOR VISUOMOTOR CONTROL AND PLANNING
            </a><br>
            <em>TL;DR:</em> Finetunes the Cosmos Video Foundation Model from NVIDIA for action prediction. 
            Core idea is to inject additional modalities like future action chunks or value function estimations into latent token sequence. 
            Results on LIBERO are good and they also have real world comparisons against <a href="https://www.physicalintelligence.company/download/pi0.pdf" target="_blank">Pi0</a>.5.
          </li>
        </ul>

        <h2 id="evaluation">7. Evaluation and Benchmarking of VLAs</h2>
        <figure class="figure-container">
          <img src="images/roboarena_real2sim.png" alt="ROBOTARENA Overview">
          <figcaption>Figure 5: Overview of real2sim ROBOTARENA benchmark. 
            Image from <a href="https://openreview.net/attachment?id=OutljIofvS&name=pdf">ROBOTARENA ∞</a>.</figcaption>
        </figure>
        <p>
          As mentioned above, the current state of VLA benchmarks is quite saturated and it is hard to judge which model is actually better given the limited number of benchmarks and the fact that most papers only compare against a few other baselines.
          Luckily, there are several submission that try to bridge this gap by introducing new benchmarks for VLAs.
          Other ideas include real2sim world models to test policies in generative environments. 
          While I don't think these ideas are on a good enough level yet to be used as real alternatives yet, its a very exciting research area that I hope to see more progress in the future.
        </p>

        <p><strong>Notable papers:</strong></p>
        <ul>
          <li>
            <a href="https://openreview.net/attachment?id=OutljIofvS&name=pdf">
              ROBOTARENA ∞: UNLIMITED ROBOT BENCHMARKING VIA REAL-TO-SIM TRANSLATION
            </a><br>
            <em>TL;DR: </em> Introduces a new benchmarking framework inspired by the <a href="https://robo-arena.github.io/" target="_blank">RoboArena</a> work for real2sim setup. 
            It provides automatic environment construction and evaluation using physics engine, real-2-sim translation and human feedback. 
            They use real2sim translation pipeline with a bunch of foundation models and differentiable rendering and VLM-based task progression scores.
            Seems very interesting on first glance and I am excited to try it out myself. 
          </li>

          <li>
            <a href="https://openreview.net/attachment?id=tQJYKwc3n4&name=pdf">
              ROBOCASA365: A LARGE-SCALE SIMULATION FRAMEWORK FOR TRAINING AND BENCHMARKING GENERALIST ROBOTS
            </a><br>
            <em>TL;DR:</em> Extends the initial RoboCase sim and benchmark setup with a very diverse setup of 365 tasks across 2k+ kitchen scenes and more than 2k hours of teleop data. 
            The task setups look great and the data scale is also promising I just wish they tested more baseline policies than just 3. 
          </li>

          <li>
            <a href="https://openreview.net/attachment?id=hidBHy1CAw&name=pdf">
              WORLDGYM: WORLD MODEL AS AN ENVIRONMENT FOR POLICY EVALUATION
            </a><br>
            <em>TL;DR:</em> WorldGym proposes using an action-conditioned video generation model (world model) as an environment for evaluating robot policies, where policies are rolled out in the generated world and evaluated by a vision-language model that provides rewards. 
          </li>
        </ul>

        <h2 id="cross-action-space">8. Cross-Action-Space-Learning</h2>
        <figure class="figure-container">
          <img src="images/xvla_cross_embedding.png" alt="X-VLA Overview">
          <figcaption>Figure 6: Different Paradigms for VLAs to handle different action spaces.
            Image from <a href="https://openreview.net/attachment?id=kt51kZH4aG&name=pdf">X-VLA</a>.</figcaption>
        </figure>

        <p>
          Most VLAs still avoid pretraining on diverse action spaces, given the difficulties of getting any positive transfer results.
          Thus, it is a very exciting research area for current VLAs to improve. 
          In addition, there is a growing interesting in using human egocentric videos with action labels for pretraining VLAs. 
          Datasets like <a href="https://arxiv.org/pdf/2505.11709" target="_blank">EgoDex</a> released earlier this year now enable more research in this direction. 
          Several interesting papers are submitted this year that try to tackle this problem with different approaches.
          They either focus on architecture details of VLAs to better handle heterogeneous action spaces or use additional abstractions like motion in image-space to get better transfer.
          It's noteworthy that the recent <a href="https://arxiv.org/pdf/2510.03342" target="_blank">Gemini Robotics 1.5</a> release from DeepMind indicates that a unreleased technique called motion transfer works for them to get positive zero-shot task transfer inbetween action spaces.
          So maybe this is just a data and model scale question. Nevertheless, more research is needed to better understand and tackle these issues.
        </p>

        <p><strong>Notable papers:</strong></p>
        <ul>
          <li>
            <a href="https://openreview.net/attachment?id=kt51kZH4aG&name=pdf">
              X-VLA: SOFT-PROMPTED TRANSFORMER AS SCALABLE CROSS-EMBODIMENT VISION-LANGUAGE-ACTION MODEL
            </a><br>
            <em>TL;DR:</em> Tackles cross-action-space learning using soft-prompting tokens for different datasets. 
            These soft-prompt tokens are learnable readout-tokens for the VLA. 
            Results on LIBERO, CALVIN and SIMPLER RoboTwin and VLABench are all very good. Also very insightful scaling analysis.
          </li>
          <li>
            <a href="https://openreview.net/attachment?id=wPEIStHxYH&name=pdf">
              XR-1: TOWARDS VERSATILE VISION-LANGUAGE-ACTION MODELS VIA LEARNING UNIFIED VISION-MOTION REPRESENTATIONS
            </a><br>
            <em>TL;DR:</em>XR-1 introduces Unified Vision-Motion Codes (UVMC), a discrete latent representation that jointly encodes both visual dynamics and robotic motion using a dual-branch VQ-VAE with a shared codebook. This enables better co-pretraining from human and robot demonstrations.
            Tested on real world experiments against Groot-N.1.5 and <a href="https://www.physicalintelligence.company/download/pi0.pdf" target="_blank">Pi0</a> with good results.
          </li>
          <li>
            <a href="https://openreview.net/attachment?id=TX3oGD99CJ&name=pdf">
              HIMOE-VLA: HIERARCHICAL MIXTURE-OF-EXPERTS FOR GENERALIST VISION–LANGUAGE–ACTION POLICIES
            </a><br>
            <em>TL;DR:</em> Substitutes Pi-Style Action Expert with Hierarchical Mixture-of-Experts Transformer to better adapt to new embodiments.
            It interleaves standard blocks with two types of MoE Blocks: Action-Space MoEs and Heterogeneity Balancing MoEs to better handle different action spaces.
            Improves upon <a href="https://www.physicalintelligence.company/download/pi0.pdf" target="_blank">Pi0</a> across a range of experiments. 
          </li>
        </ul>

        <h2 id="other">9. Other Interesting Papers</h2>
        <p>
          There are several other interesting papers that don't fit neatly into the categories above, but are worth mentioning.
          These papers explore various aspects of VLA models, from choice of VLM backbone to including a memory module in the VLA architecture. 
          I especially like the memory idea, as most VLAs just encode the current image and ignore prior timesteps, which is a big limitation for many tasks.
          However natively adding long history does not work well, because they overfit too strong on the history. During rollouts, its unlikely to encounter the same sequence of states than human demonstrators which result in low performance.
          Thus, memory modules that aggregate prior timesteps in a more general way are a very exciting research area.
        </p>

        <p><strong>Notable papers:</strong></p>
        <ul>
          <li>
            <a href="https://openreview.net/attachment?id=KcJ9U0x6kO&name=pdf">
              HAMLET: SWITCH YOUR VISION-LANGUAGE-ACTION MODEL INTO A HISTORY-AWARE POLICY
            </a><br>
            <em>TL;DR:</em> Introduces a plug-and-play style memory module with moment tokens to capture temporal stuff from prior timesteps. A proposed memory module aggregates tokens over time to enable history-conditioned prediction.
          </li>

          <li>
            <a href="https://openreview.net/attachment?id=tc2UsBeODW&name=pdf">
              VLM4VLA: REVISITING VISION-LANGUAGE-MODELS IN VISION-LANGUAGE-ACTION MODELS
            </a><br>
            <em>TL;DR:</em> Compares a lot of VLMs as backbone choice for VLAs and finds that downstream performance has no correlation with VLM performance on standard benchmarks.
            This confirms my own experience in experimenting with various VLM backbones. However, the paper is still limited to benchmark setups only and does not test real robot results.
          </li>
        </ul>
          
        <h2 id="frontier-gap">10. The Hidden Gap Between Frontier and Research VLAs</h2>
        <figure class="figure-container">
          <img src="images/closed_vs_open_VLAs.png" alt="VLAs Overview">
          <figcaption>Figure 7: Overview of closed-weight vs open-weight VLAs.</a>.</figcaption>
        </figure>
        <p>
          On paper (pun intended:)), the gap seems small: on simulation setups (LIBERO, CALVIN) open-source VLAs surpass popular frontier baselines like Pi0.5. 
          In practice, there is still a bigger gap that only shows up precisely where current papers rarely evaluate: <em>zero-shot, open-world behavior after pretraining</em>. 
          Two weeks ago at CoRL, the Gemini-Robotics VLA demo attempted a wide range of novel tasks with arbitrary objects and paraphrased language. 
          My own VLA  <a href="https://arxiv.org/pdf/2509.04996">FLOWER</a> is state-of-the-art on CALVIN benchmark, but nowhere close to that level of zero-shot robustness. 
          Simulation Benchmarks hide this delta and current simulation setups don't optimize for this objective.<br><br>
          
          This gap is not unique to VLAs, the same is true for LLMs and VLMs.
          For VLAs, fully open-weight models (that share training recipe, data, code and weights) still lag significantly behind closed-weight models like Gemini-Robotics and Pi0.5 on zero-shot tasks.
          (although there seem to be some promising signs that this gap is closing with <a href="https://arxiv.org/abs/2508.21112">EO-1</a> but I haven't tried them myself yet).
          This is not to say that open-weight VLAs are useless, they are very useful for research and have strong performance in a lot of scenarios.
          But I wanted to highlight this gap, as I believe it is a significant issue for our community that needs to be addressed in the future.<br><br>
        </p>
      <p><strong>Why the gap exists (based on what’s visible from papers, discussion with peers and personal experience):</strong></p>
      <ul>
        <li><em>Benchmark progress saturation masks real progress.</em> When scores cluster near the ceiling, “+0.5%” is not evidence of real improvements.</li>
        <li><em>High quality Data Gap</em> The current available open-source data is limited in diversity and scale, which limits the training of more general models.</li>
        <li><em>Understanding of High Quality Data</em>It's not only the data scale its also about the missing knowledge gap of understanding high quality demonstration data.</li>
        <li><em>Evaluation scope is narrow.</em> Most papers report sim-only or small, locally finetuned setups. 
          Free-form zero-shot language following and unseen objects/rooms are very rare. 
          However, it's important to highlight that proper large-scale evals are only doable by companies.</li>
        <li><em>Operational constraints.</em> Research groups lack the manpower/time to run large, diverse real-world trials necessary for rapid iteration. 
          Frontier labs operate at a different scale of manpower, funding, and robot fleet size.
        Thus, many PhDs, including myself, iterate on simulation benchmarks to test and try out their ideas.</li>
        <li><em>Missing Peer Review Incentives.</em> Reviewers at most major conferences expect head-to-head comparison against open-weight but closed-source trained VLAs on standard sims and strong local finetuning numbers. 
          These benchmarks are good for paper acceptance, but weakly correlated with open-world performance.</li>
      </ul>
      <p><strong>Counterarguments against this thesis:</strong></p>
      <ul>
        <li><em>Research Findings are not equal to performance gains.</em>
          Many great papers I discussed here have general interesting findings useful for the community and research. 
          Zero-shot Performance is only one aspect of VLA Performance.</li>
      </ul>
      If you have good arguments for or against this thesis, please reach out to me, very happy to hear other opinions.<br><br>
      <figure class="figure-container">
        <img src="images/roboarena_leaderboard.png" alt="Zero-Shot Gap Illustration">
        <figcaption>Figure 7: Overview of the current <a href="https://robo-arena.github.io/" target="_blank">RoboArena</a> Leaderboard. There is only one non-Pi model that is near to being competitive in zero-shot tasks.</figcaption>
      </figure>
      <p><strong>What would help to bridge this gap without blowing the compute and human manpower budget:</strong></p>
      <ul>
        <li><em>Usage of public zero-shot fair benchmarks.</em> Track progress on setups like <a href="https://robo-arena.github.io/" target="_blank">RoboArena</a> that test post-pretraining generalization by independent operators. 
          Today, non-Pi models are scarcely represented and far behind. However, the training code for the higher ranked policies are all fully open-sourced by PI in this <a href="https://github.com/Physical-Intelligence/openpi" target="_blank">codebase</a> to facilitate further research.</li> 
        <li><em>Better Pretraining Recipes.”</em> I've seen only very few papers like <a href="https://openreview.net/attachment?id=kt51kZH4aG&name=pdf">X-VLA</a> that properly ablate all their pretraining design decisions to understand the impact of these for performance. 
        We need more papers like this that share full pretraining recipes and also discuss failed ideas that did not work during pretraining.</li>
        </ul>
      <p>
        I wanna clearly highlight, that I don't think simulation or local finetuning are useless for research in contrast they are very important for many parts of robot learning. 
        Only that they are a poor proxy for the thing for the main argument of VLA: <em>robust zero-shot behavior in messy, new environments</em>. 
      </p>

        
        <h2 id="summary-outlook">11. Summary and Outlook</h2>

        <p>
          Overall, I am very positive about the current state of VLA research and the progress being made in this field.
          The trends above show strong interest and contributions across VLA models—from architecture design to training strategies and evaluation methods.<br><br>

          However, there are also a few disappointing aspects of the current state of VLA research, apart from the zero-shot gap.<br><br>

          My biggest complaint for the ICLR submissions is the lack of data-focused papers.
          Given the importance of data quality and diversity for training VLAs, I had hoped to see more work in this direction.
          I believe this remains one of the least understood areas of VLA research, and we need better methods to collect and curate high-quality datasets.
          That said, data-centric research is hard.
          It’s an open secret that OXE is mostly low-quality data, and there are only a few large-scale, high-quality datasets.
          Understanding how to quantify data quality in imitation learning is one of the most crucial open problems in VLA research.<br><br>

          Another interesting direction I was not able to find relevant submissions is in-context learning for VLAs.
          I believe there is a lot of potential in this direction, given the success of in-context learning for LLMs and VLMs.
          But its unclear to how to best implement this for VLAs so far.
          There have been good attempts but I think this could be key to enable easy prompting and zero-shot task generalization for VLAs.
          Language is great as an universal interface to communicate tasks but it also lacks a lot of information and context that is needed to solve complex tasks.
          I hope to see more research in this direction in the next year.<br><br>

          Nevertheless, I am optimistic that the field will continue to grow and evolve rapidly. 
        </p>
        
        <h2 id="cite">Cite this post</h2>
        <p>If you’d like to cite this article, use the BibTeX below:</p>
        <pre><code>@misc{reuss2025state-vla-iclr26,
          title        = {State of VLA Research at ICLR 2026},
          author       = {Reuss, Moritz},
          year         = {2025},
          month        = {October},
          howpublished = {\url{https://mbreuss.github.io/blog_post_iclr_26_vla.html}},
          note         = {Blog post},
        }</code></pre>

        
        <hr>
        <p style="text-align:center;">
          <a href="index.html">← Back to main page</a>
        </p>

      </td>
    </tr>
  </table>
</body>
</html>
