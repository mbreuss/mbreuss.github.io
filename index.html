<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

  <title>Moritz Reuss ‚Äî Robotics & VLA Research</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="author" content="Moritz Reuss" />
  <meta name="description" content="Moritz Reuss ‚Äî PhD at KIT IRL. Vision-Language-Action (VLA) models, diffusion/flow policies, efficient generalist robot learning. Publications, projects, and writing." />

  <!-- Canonical -->
  <link rel="canonical" href="https://mbreuss.github.io/" />

  <!-- Favicon -->
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">

  <!-- Styles -->
  <link rel="stylesheet" type="text/css" href="stylesheet.css" />

  <!-- Open Graph -->
  <meta property="og:title" content="Moritz Reuss ‚Äî Robotics & VLA Research" />
  <meta property="og:description" content="Vision-Language-Action (VLA) models, diffusion/flow policies, efficient generalist robot learning. Publications, projects, and writing." />
  <meta property="og:type" content="website" />
  <meta property="og:url" content="https://mbreuss.github.io/" />
  <meta property="og:image" content="https://mbreuss.github.io/images/Moritz_circle.jpeg" />
  <meta property="og:site_name" content="Moritz Reuss" />

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="Moritz Reuss ‚Äî Robotics & VLA Research" />
  <meta name="twitter:description" content="Vision-Language-Action (VLA) models, diffusion/flow policies, efficient generalist robot learning. Publications, projects, and writing." />
  <meta name="twitter:image" content="https://mbreuss.github.io/images/Moritz_circle.jpeg" />

  <!-- JSON-LD: Person -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Person",
    "name": "Moritz Reuss",
    "url": "https://mbreuss.github.io/",
    "image": "https://mbreuss.github.io/images/Moritz_circle.jpeg",
    "jobTitle": "PhD Researcher",
    "affiliation": {
      "@type": "Organization",
      "name": "Karlsruhe Institute of Technology (KIT) ‚Äî Intuitive Robots Lab"
    },
    "sameAs": [
      "mailto:reussmoritz@gmail.com",
      "https://scholar.google.com/citations?user=NLuzkPIAAAAJ",
      "https://github.com/mbreuss/",
      "https://www.linkedin.com/in/moritzreuss/?locale=en_US"
    ]
  }
  </script>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-ETMW7BYE0C"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-ETMW7BYE0C');
  </script>
  <!-- End Google Analytics -->
</head>

<body>
  <!-- Outer container (kept table structure for your CSS) -->
  <table role="main" style="width:100%;max-width:800px;border:0;border-spacing:0;border-collapse:separate;margin:0 auto;">
    <tbody>
      <tr>
        <td>

          <!-- Header / Bio -->
          <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin:0 auto;">
            <tbody>
              <tr>
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Moritz Reuss</name>
                  </p>

                  <p>
                    I am a fourth-year PhD student in the
                    <a href="https://irl.anthropomatik.kit.edu/" target="_blank" rel="noopener noreferrer">Intuitive Robots Lab</a> (IRL)
                    at the Karlsruhe Institute of Technology (KIT), Germany.
                    My research focuses on developing machine learning methods to teach robots new behaviors from uncurated, multimodal human demonstrations,
                    supervised by <a href="http://rudolf.intuitive-robots.net/" target="_blank" rel="noopener noreferrer">Rudolf Lioutikov</a>.
                    I am grateful for the
                    <a href="https://machinelearning.apple.com/updates/apple-scholars-aiml-2025/" target="_blank" rel="noopener noreferrer">Apple PhD Fellowship 2025</a>
                    for supporting my research. I was an intern at Apple DMLI, working on robotics with
                    <a href="https://peidehuang.github.io/" target="_blank" rel="noopener noreferrer">Peide Huang</a> and
                    <a href="https://www.linkedin.com/in/jianzhangpurdue" target="_blank" rel="noopener noreferrer">Jian Zhang</a>.
                    Previously, I obtained my Master's degree in Mechanical Engineering at KIT, where I wrote my thesis at Bosch Research,
                    supervised by <a href="https://alr.anthropomatik.kit.edu/21_65.php" target="_blank" rel="noopener noreferrer">Gerhard Neumann</a>.
                    During my studies I interned at Audi AG, IPG Automotive, and the Research Center for Informatics (FZI).
                  </p>

                  <p style="text-align:center">
                    <a href="mailto:reussmoritz@gmail.com">Email</a> &nbsp;/&nbsp;
                    <a href="data/CV_Moritz_Reuss.pdf" target="_blank" rel="noopener noreferrer">CV</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?user=NLuzkPIAAAAJ&hl=de" target="_blank" rel="noopener noreferrer">Google Scholar</a> &nbsp;/&nbsp;
                    <a href="https://github.com/mbreuss/" target="_blank" rel="noopener noreferrer">GitHub</a> &nbsp;/&nbsp;
                    <a href="https://www.linkedin.com/in/moritzreuss/?locale=en_US" target="_blank" rel="noopener noreferrer">LinkedIn</a> &nbsp;/&nbsp;
                    <a href="https://mbreuss.github.io/blog_post_iclr_26_vla.html">Blog</a>
                  </p>
                </td>

                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/Moritz_circle.jpeg">
                    <img
                      src="images/Moritz_circle.jpeg"
                      alt="Portrait of Moritz Reuss"
                      style="width:100%;max-width:100%"
                      class="hoverZoomLink"
                      loading="lazy"
                    />
                  </a>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Research (now first) -->
          <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin:0 auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                  <p>
                    My primary research goal is to build intelligent embodied agents that assist people in their everyday lives and
                    communicate intuitively. I focus on language-conditioned multitask imitation learning from robot play data.
                    My work explores efficient Vision-Language-Action (VLA) policies and novel policy representations
                    that can learn from uncurated, multimodal human demonstrations without rewards.
                    I have worked on score/flow-based diffusion policies and developed generalist VLA models that achieve strong performance with
                    minimal computational requirements. Representative papers are <span class="highlight">highlighted</span>.
                  </p>

                  <!-- Short, precise sub-topics -->
                  <ul style="margin-top:8px;">
                  <li><strong>Efficient VLAs:</strong> Compact flow/diffusion-based vision-language-action models (e.g., <em>FLOWER</em>, <em>MoDE</em>) and novel efficient action tokenization methods (e.g., <em>BEAST</em>) that achieve strong performance with minimal computational requirements.</li>
                  
                  <li><strong>Expressive Policy Representations:</strong> Diffusion and flow-based policies that capture multimodal action distributions and handle diverse goal specifications including language, images, and waypoints (e.g., <em>BESO</em>, <em>MDT</em>), with benchmarks for evaluating behavior diversity (e.g., <em>D3IL</em>).</li>
                  
                  <li><strong>Learning from Uncurated Data:</strong> Zero-shot dataset labeling with vision-language foundation models (e.g., <em>NILS</em>) to enable scalable learning from robot play data without manual annotation, and methods for handling multimodal observations (RGB + 3D inputs).</li>
                </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Writing (moved below Research) -->
          <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin:0 auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <div style="margin:0; padding:12px 16px; background:#f8fafc; border-left:4px solid #4CAF50;">
                    <heading>Writing</heading>
                    <p style="margin-top:6px; margin-bottom:0;">
                      <a href="https://mbreuss.github.io/blog_post_iclr_26_vla.html">
                        State of Vision-Language-Action (VLA) Research at ICLR 2026
                      </a>
                      <span style="color:#4b5563;">(Oct 2025)</span>
                    </p>
                  </div>
                </td>
              </tr>
            </tbody>
          </table>


          <!-- Papers list -->
          <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin:0 auto;">
            <heading>Research Papers</heading>
            <tbody>

              <!-- FLOWER (highlighted) -->
              <tr style="background-color:#ffffd0;">
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <div style="display:flex;">
                    <div style="flex:0 0 25%;max-width:25%;">
                      <img src="images/flower_overview.png" alt="FLOWER VLA overview diagram" style="width:100%;max-width:100%;" loading="lazy" />
                    </div>
                    <div style="flex:0 0 75%;max-width:75%;padding-left:20px;">
                      <a href="https://www.arxiv.org/pdf/2509.04996" target="_blank" rel="noopener noreferrer">
                        <papertitle>FLOWER: Democratizing Generalist Robot Policies with Efficient Vision-Language-Action Flow Policies</papertitle>
                      </a>
                      <br />
                      <strong>Moritz Reuss</strong>,
                      Hongyi Zhou,
                      Marcel R√ºhle,
                      √ñmer Erdin√ß Yaƒümurlu,
                      Fabian Otto,
                      <a href="http://rudolf.intuitive-robots.net/" target="_blank" rel="noopener noreferrer">Rudolf Lioutikov</a>
                      <br /><br />
                      <em>CoRL</em>, 2025
                      <br />
                      <a href="https://github.com/intuitive-robots/flower_vla_calvin/tree/main" target="_blank" rel="noopener noreferrer">Project Page</a> /
                      <a href="https://intuitive-robots.github.io/flower_vla/" target="_blank" rel="noopener noreferrer">Code</a> /
                      <a href="https://www.arxiv.org/pdf/2509.04996" target="_blank" rel="noopener noreferrer">arXiv</a>
                      <p></p>
                      <p>
                        We systematically analyze VLA design decisions for small and efficient VLAs. Our findings lead us to introduce FLOWER, a 950M-parameter
                        Vision-Language-Action (VLA) policy that achieves state-of-the-art performance across 190 tasks in 10 benchmarks while requiring only
                        ~1% of the pretraining compute of models like OpenVLA. FLOWER introduces intermediate-modality fusion and action-specific Global-AdaLN
                        conditioning to achieve strong performance with improved efficiency. Our approach democratizes VLA development by making high-performance
                        robotic foundation models accessible with commodity hardware, requiring significantly less GPU memory to run.
                      </p>
                    </div>
                  </div>
                </td>
              </tr>

              <!-- BEAST -->
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <div style="display:flex;">
                    <div style="flex:0 0 25%;max-width:25%;">
                      <img src="images/beast_overview.png" alt="BEAST B-spline action tokenizer overview" style="width:100%;max-width:100%;" loading="lazy" />
                    </div>
                    <div style="flex:0 0 75%;max-width:75%;padding-left:20px;">
                      <a href="https://arxiv.org/pdf/2506.06072" target="_blank" rel="noopener noreferrer">
                        <papertitle>BEAST: An Efficient Action Tokenizer with B-Splines</papertitle>
                      </a>
                      <br />
                      Hongyi Zhou,
                      Weiran Liao,
                      Xi Huang,
                      Yucheng Tang,
                      Fabian Otto,
                      <a href="https://irl.anthropomatik.kit.edu/21_78.php" target="_blank" rel="noopener noreferrer">Xiaogang Jia</a>,
                      Xinkai Jiang,
                      Simon Hilber,
                      Ge Li,
                      Qian Wang,
                      √ñmer Erdin√ß Yaƒümurlu,
                      Nils Blank,
                      <strong>Moritz Reuss</strong>,
                      <a href="http://rudolf.intuitive-robots.net/" target="_blank" rel="noopener noreferrer">Rudolf Lioutikov</a>
                      <br /><br />
                      <em>NeurIPS</em>, 2025
                      <br />
                      <a href="https://beast-tokenizer.github.io/" target="_blank" rel="noopener noreferrer">Project Page</a> /
                      <a href="https://github.com/intuitive-robots/beast_calvin" target="_blank" rel="noopener noreferrer">Code</a> /
                      <a href="https://arxiv.org/pdf/2506.06072" target="_blank" rel="noopener noreferrer">arXiv</a>
                      <p></p>
                      <p>
                        We introduce BEAST, a B-spline‚Äìbased action tokenizer that efficiently represents continuous robot actions for generalist policies
                        while maintaining smooth trajectories essential for robot control. BEAST enables efficient action representation and improved performance
                        in VLA models by leveraging the mathematical properties of B-splines for smooth, continuous control. It flexibly supports continuous tokens
                        and discrete tokenization. Experiments across various benchmarks verify good compression with strong performance and smooth behavior
                        without additional temporal aggregation.
                      </p>
                    </div>
                  </div>
                </td>
              </tr>

              <!-- PointMapPolicy -->
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <div style="display:flex;">
                    <div style="flex:0 0 25%;max-width:25%;">
                      <img src="images/Point_map.png" alt="PointMapPolicy: structured point cloud grid for diffusion policy" style="width:100%;max-width:100%;" loading="lazy" />
                    </div>
                    <div style="flex:0 0 75%;max-width:75%;padding-left:20px;">
                      <a href="https://openreview.net/pdf?id=ZR2mdBrhJX" target="_blank" rel="noopener noreferrer">
                        <papertitle>PointMapPolicy: Structured Point Cloud Processing for Multi-Modal Imitation Learning</papertitle>
                      </a>
                      <br />
                      <a href="https://irl.anthropomatik.kit.edu/21_78.php" target="_blank" rel="noopener noreferrer">Xiaogang Jia</a>,
                      Qian Wang,
                      Anrui Wang,
                      Han A. Wang,
                      Bal√°zs Gyenes,
                      Emiliyan Gospodinov,
                      Xinkai Jiang,
                      Ge Li,
                      Hongyi Zhou,
                      Weiran Liao,
                      Xi Huang,
                      Maximilian Beck,
                      <strong>Moritz Reuss</strong>,
                      <a href="http://rudolf.intuitive-robots.net/" target="_blank" rel="noopener noreferrer">Rudolf Lioutikov</a>,
                      Gerhard Neumann
                      <br /><br />
                      <em>NeurIPS</em>, 2025
                      <br />
                      <a href="https://point-map.github.io/Point-Map/" target="_blank" rel="noopener noreferrer">Project Page</a> /
                      <a href="https://openreview.net/pdf?id=ZR2mdBrhJX" target="_blank" rel="noopener noreferrer">Paper</a>
                      <p></p>
                      <p>
                        We present PointMapPolicy, a multi-modal imitation learning method that conditions diffusion policies on structured grids of points
                        without downsampling. Our approach fuses point maps with RGB using an xLSTM backbone, enabling direct application of computer vision
                        techniques to 3D data while preserving fine-grained geometric details. On RoboCasa and CALVIN, plus real-robot evaluations, we achieve
                        state-of-the-art performance across diverse manipulation tasks.
                      </p>
                    </div>
                  </div>
                </td>
              </tr>

              <!-- MoDE (highlighted) -->
              <tr style="background-color:#ffffd0;">
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <div style="display:flex;">
                    <div style="flex:0 0 25%;max-width:25%;">
                      <img src="images/MoDE_X.png" alt="MoDE: mixture-of-denoising experts diffusion transformer policy" style="width:100%;max-width:100%;" loading="lazy" />
                    </div>
                    <div style="flex:0 0 75%;max-width:75%;padding-left:20px;">
                      <a href="https://openreview.net/pdf?id=nDmwloEl3N" target="_blank" rel="noopener noreferrer">
                        <papertitle>Efficient Diffusion Transformer Policies with Mixture of Expert Denoisers for Multitask Learning</papertitle>
                      </a>
                      <br />
                      <strong>Moritz Reuss*</strong>,
                      Jyothish Pari*,
                      <a href="https://people.csail.mit.edu/pulkitag/" target="_blank" rel="noopener noreferrer">Pulkit Agrawal</a>,
                      <a href="http://rudolf.intuitive-robots.net/" target="_blank" rel="noopener noreferrer">Rudolf Lioutikov</a>
                      <br /><br />
                      <em>ICLR</em>, 2025
                      <br />
                      <a href="https://mbreuss.github.io/MoDE_Diffusion_Policy/" target="_blank" rel="noopener noreferrer">Project Page</a> /
                      <a href="https://github.com/intuitive-robots/MoDE_Diffusion_Policy" target="_blank" rel="noopener noreferrer">Code</a> /
                      <a href="https://arxiv.org/pdf/2412.12953" target="_blank" rel="noopener noreferrer">arXiv</a>
                      <p></p>
                      <p>
                        We propose Mixture-of-Denoising Experts (MoDE), a generalist policy for guided behavior generation that outperforms dense transformer-based
                        diffusion policies in accuracy, parameter count, and efficiency. Our routing strategy conditions expert selection on the current noise level
                        of the diffusion process. On four imitation learning benchmarks (including CALVIN and LIBERO), MoDE consistently exceeds dense transformer
                        baselines. We pretrain MoDE on a subset of OXE for just 3 days on 6 GPUs and surpass OpenVLA and Octo on SIMPLER. MoDE achieves higher average
                        performance with ~90% fewer FLOPs, ~20% faster inference, and ~40% fewer parameters compared to dense transformer diffusion policies.
                      </p>
                    </div>
                  </div>
                </td>
              </tr>

              <!-- LUPUS -->
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <div style="display:flex;">
                    <div style="flex:0 0 25%;max-width:25%;">
                      <img src="images/LUPUS_Overview_Figure.png" alt="Zero-shot relabeling pipeline with foundation models (LUPUS)" style="width:100%;max-width:100%;" loading="lazy" />
                    </div>
                    <div style="flex:0 0 75%;max-width:75%;padding-left:20px;">
                      <a href="https://robottasklabeling.github.io/" target="_blank" rel="noopener noreferrer">
                        <papertitle>Scaling Robot Policy Learning via Zero-Shot Labeling with Foundation Models</papertitle>
                      </a>
                      <br />
                      Nils Blank,
                      <strong>Moritz Reuss</strong>,
                      Marcel R√ºhle,
                      √ñmer Erdin√ß Yaƒümurlu,
                      Fabian Wenzel,
                      <a href="https://www.oiermees.com/" target="_blank" rel="noopener noreferrer">Oier Mees</a>,
                      <a href="http://rudolf.intuitive-robots.net/" target="_blank" rel="noopener noreferrer">Rudolf Lioutikov</a>
                      <br /><br />
                      Conference on Robot Learning (<em>CoRL</em>), 2024 ‚Äî Oral @ 2nd Workshop on Mobile Manipulation and Embodied Intelligence, <em>ICRA 2024</em>
                      <br />
                      <a href="https://openreview.net/pdf?id=EdVNB2kHv1" target="_blank" rel="noopener noreferrer">Paper</a>
                      <p></p>
                      <p>
                        We introduce a method to automatically label uncurated, long-horizon robot teleoperation data at scale in a zero-shot manner without human intervention.
                        We combine pre-trained vision-language foundation models to detect objects, propose tasks, segment tasks in large unlabeled interaction datasets,
                        and train language-conditioned policies on the relabeled data. Initial experiments show that our method enables policies that match those trained
                        with oracle human annotations.
                      </p>
                    </div>
                  </div>
                </td>
              </tr>

              <!-- MDT (highlighted) -->
              <tr style="background-color:#ffffd0;">
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <div style="display:flex;">
                    <div style="flex:0 0 25%;max-width:25%;">
                      <img src="images/calvin_mdt.gif" alt="MDT: multimodal diffusion transformer on CALVIN" style="width:100%;max-width:100%;" loading="lazy" />
                    </div>
                    <div style="flex:0 0 75%;max-width:75%;padding-left:20px;">
                      <a href="https://intuitive-robots.github.io/mdt_policy/" target="_blank" rel="noopener noreferrer">
                        <papertitle>Multimodal Diffusion Transformer: Learning Versatile Behavior from Multimodal Goals</papertitle>
                      </a>
                      <br />
                      <strong>Moritz Reuss</strong>,
                      √ñmer Erdin√ß Yaƒümurlu,
                      Fabian Wenzel,
                      <a href="http://rudolf.intuitive-robots.net/" target="_blank" rel="noopener noreferrer">Rudolf Lioutikov</a>
                      <br /><br />
                      <em>Robotics: Science and Systems (RSS)</em>, 2024 ‚Äî <em><span style="color:red;">Oral</span></em> @ Workshop on Language and Robot Learning (LangRob), <em>CoRL 2023</em>
                      <br />
                      <a href="https://intuitive-robots.github.io/mdt_policy/" target="_blank" rel="noopener noreferrer">Project Page</a> /
                      <a href="https://github.com/intuitive-robots/mdt_policy" target="_blank" rel="noopener noreferrer">Code</a> /
                      <a href="https://arxiv.org/pdf/2407.05996" target="_blank" rel="noopener noreferrer">arXiv</a>
                      <p></p>
                      <p>
                        We present a diffusion policy for learning from uncurated, reward-free offline data with sparse language labels.
                        Multimodal Diffusion Transformer (MDT) learns complex, long-horizon behaviors and sets a new state of the art on CALVIN.
                        MDT leverages pre-trained vision and language foundation models and aligns multimodal goal specifications in the transformer encoder‚Äôs latent space,
                        using two self-supervised auxiliary objectives to better follow goals specified in language and images.
                      </p>
                    </div>
                  </div>
                </td>
              </tr>

              <!-- D3IL -->
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <div style="display:flex;">
                    <div style="flex:0 0 25%;max-width:25%;">
                      <img src="images/d3il_compressed.gif" alt="D3IL benchmark environments for imitation learning" style="width:100%;max-width:100%;" loading="lazy" />
                    </div>
                    <div style="flex:0 0 75%;max-width:75%;padding-left:20px;">
                      <a href="https://openreview.net/pdf?id=6pPYRXKPpw" target="_blank" rel="noopener noreferrer">
                        <papertitle>Towards Diverse Behaviors: A Benchmark for Imitation Learning with Human Demonstrations</papertitle>
                      </a>
                      <br />
                      <a href="https://irl.anthropomatik.kit.edu/21_78.php" target="_blank" rel="noopener noreferrer">Xiaogang Jia</a>,
                      <a href="https://alr.anthropomatik.kit.edu/21_495.php" target="_blank" rel="noopener noreferrer">Denis Blessing</a>,
                      <a href="https://alr.iar.kit.edu/21_500.php" target="_blank" rel="noopener noreferrer">Xinkai Jiang</a>,
                      <strong>Moritz Reuss</strong>,
                      Atalay Donat,
                      <a href="http://rudolf.intuitive-robots.net/" target="_blank" rel="noopener noreferrer">Rudolf Lioutikov</a>,
                      <a href="https://alr.anthropomatik.kit.edu/21_65.php" target="_blank" rel="noopener noreferrer">Gerhard Neumann</a>
                      <br /><br />
                      <em>ICLR</em>, 2024
                      <br />
                      <a href="https://openreview.net/pdf?id=6pPYRXKPpw" target="_blank" rel="noopener noreferrer">OpenReview</a>
                      <p></p>
                      <p>
                        D3IL introduces simulation benchmark environments and datasets tailored for imitation learning, designed to evaluate a model‚Äôs ability to learn
                        and replicate diverse, multimodal human behaviors. Environments encompass multiple sub-tasks and object manipulations, providing rich diversity
                        often lacking in other datasets. We also propose practical metrics to quantify behavioral diversity and benchmark state-of-the-art methods.
                      </p>
                    </div>
                  </div>
                </td>
              </tr>

              <!-- BESO (highlighted) -->
              <tr style="background-color:#ffffd0;">
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <div style="display:flex;">
                    <div style="flex:0 0 25%;max-width:25%;">
                      <img src="images/beso_kitchen.gif" alt="BESO: goal-conditioned diffusion policy" style="width:100%;max-width:100%;" loading="lazy" />
                    </div>
                    <div style="flex:0 0 75%;max-width:75%;padding-left:20px;">
                      <a href="https://intuitive-robots.github.io/beso-website" target="_blank" rel="noopener noreferrer">
                        <papertitle>Goal-Conditioned Imitation Learning using Score-based Diffusion Policies</papertitle>
                      </a>
                      <br />
                      <strong>Moritz Reuss</strong>,
                      <a href="https://irl.anthropomatik.kit.edu/21_67.php" target="_blank" rel="noopener noreferrer">Maximilian Li</a>,
                      <a href="https://irl.anthropomatik.kit.edu/21_78.php" target="_blank" rel="noopener noreferrer">Xiaogang Jia</a>,
                      <a href="http://rudolf.intuitive-robots.net/" target="_blank" rel="noopener noreferrer">Rudolf Lioutikov</a>
                      <br /><br />
                      <em><span style="color:red; font-weight:bold;">Best Paper Award</span> @ Workshop on Learning from Diverse, Offline Data (L-DOD), <em>ICRA 2023</em>; <em>RSS</em>, 2023</em>
                      <br />
                      <a href="https://intuitive-robots.github.io/beso-website" target="_blank" rel="noopener noreferrer">Project Page</a> /
                      <a href="https://github.com/intuitive-robots/beso" target="_blank" rel="noopener noreferrer">Code</a> /
                      <a href="https://arxiv.org/pdf/2304.02532" target="_blank" rel="noopener noreferrer">arXiv</a>
                      <p></p>
                      <p>
                        We present BESO, a policy representation for goal-conditioned imitation learning using score-based diffusion models.
                        BESO effectively learns goal-directed, multimodal behavior from uncurated, reward-free offline data and achieves state of the art
                        with as few as three denoising steps.
                      </p>
                    </div>
                  </div>
                </td>
              </tr>

              <!-- IMC -->
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <div style="display:flex;">
                    <div style="flex:0 0 25%;max-width:25%;">
                      <img src="images/IMC_obstacle_avoidance.png" alt="Information Maximizing Curriculum: obstacle avoidance result" style="width:100%;max-width:100%;" loading="lazy" />
                    </div>
                    <div style="flex:0 0 75%;max-width:75%;padding-left:20px;">
                      <a href="https://arxiv.org/pdf/2303.15349" target="_blank" rel="noopener noreferrer">
                        <papertitle>Information Maximizing Curriculum: A Curriculum-Based Approach for Learning Versatile Skills</papertitle>
                      </a>
                      <br />
                      <a href="https://alr.anthropomatik.kit.edu/21_495.php" target="_blank" rel="noopener noreferrer">Denis Blessing</a>,
                      <a href="https://alr.anthropomatik.kit.edu/21_69.php" target="_blank" rel="noopener noreferrer">Onur Celik</a>,
                      <a href="https://irl.anthropomatik.kit.edu/21_78.php" target="_blank" rel="noopener noreferrer">Xiaogang Jia</a>,
                      <strong>Moritz Reuss</strong>,
                      <a href="https://alr.anthropomatik.kit.edu/21_67.php" target="_blank" rel="noopener noreferrer">Maximilian Xiling</a>,
                      <a href="http://rudolf.intuitive-robots.net/" target="_blank" rel="noopener noreferrer">Rudolf Lioutikov</a>,
                      <a href="https://alr.anthropomatik.kit.edu/21_65.php" target="_blank" rel="noopener noreferrer">Gerhard Neumann</a>
                      <br /><br />
                      <em>NeurIPS</em>, 2023
                      <br />
                      <a href="https://arxiv.org/pdf/2303.15349" target="_blank" rel="noopener noreferrer">arXiv</a>
                      <p></p>
                      <p>
                        We introduce the Information Maximizing Curriculum to address mode-averaging in imitation learning by enabling specialization
                        on representable data. A mixture-of-experts policy focuses on different data subsets, with a maximum-entropy objective for full dataset coverage.
                      </p>
                    </div>
                  </div>
                </td>
              </tr>

              <!-- Hybrid Inverse Dynamics (highlighted) -->
              <tr style="background-color:#ffffd0;">
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <div style="display:flex;">
                    <div style="flex:0 0 25%;max-width:25%;">
                      <img src="images/panda_dance_scene.gif" alt="Hybrid inverse dynamics model on Franka Emika Panda" style="width:100%;max-width:100%;" loading="lazy" />
                    </div>
                    <div style="flex:0 0 75%;max-width:75%;padding-left:20px;">
                      <a href="https://arxiv.org/pdf/2205.13804" target="_blank" rel="noopener noreferrer">
                        <papertitle>End-to-End Learning of Hybrid Inverse Dynamics Models for Precise and Compliant Impedance Control</papertitle>
                      </a>
                      <br />
                      <strong>Moritz Reuss</strong>,
                      <a href="https://scholar.google.be/citations?user=OD-ysAcAAAAJ&hl=nl" target="_blank" rel="noopener noreferrer">Niels van Duijkeren</a>,
                      <a href="https://scholar.google.be/citations?user=OZNzz9gAAAAJ&hl=nl" target="_blank" rel="noopener noreferrer">Robert Krug</a>,
                      <a href="https://alr.anthropomatik.kit.edu/21_72.php" target="_blank" rel="noopener noreferrer">Philipp Becker</a>,
                      <a href="https://alr.anthropomatik.kit.edu/21_224.php" target="_blank" rel="noopener noreferrer">Vaisakh Shaj</a>,
                      <a href="https://alr.anthropomatik.kit.edu/21_65.php" target="_blank" rel="noopener noreferrer">Gerhard Neumann</a>
                      <br /><br />
                      <em>Robotics: Science and Systems (RSS)</em>, 2022
                      <br />
                      <a href="https://arxiv.org/pdf/2205.13804" target="_blank" rel="noopener noreferrer">arXiv</a>
                      <p></p>
                      <p>
                        We present a hybrid model combining a differentiable rigid-body model with a recurrent LSTM to accurately model the inverse dynamics
                        of a robot manipulator. A differentiable formulation of barycentric parameters enables end-to-end training jointly with the residual network
                        while preserving physical consistency. On a Franka Emika Panda, the model enables precise and compliant motion tracking.
                      </p>
                    </div>
                  </div>
                </td>
              </tr>

            </tbody>
          </table>

          <!-- Footer -->
          <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin:0 auto;">
            <tbody>
              <tr>
                <td style="padding:0; vertical-align: middle;">
                  <br />
                  <p style="text-align:right;font-size:small;color:#4b5563;">
                    The website is based on the
                    <a href="https://github.com/jonbarron/jonbarron_website" target="_blank" rel="noopener noreferrer">source code</a>.
                    &nbsp;|&nbsp;
                    <a href="https://mbreuss.github.io/blog_post_iclr_26_vla.html">Blog</a>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

        </td>
      </tr>
    </tbody>
  </table>
</body>
</html>
